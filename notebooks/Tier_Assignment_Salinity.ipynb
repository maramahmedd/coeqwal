{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a40811-1df1-4f6e-82fa-715cb0252d1a",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32c3ca-69ac-43cb-bf7a-d31e6a804ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import sys\n",
    "sys.path.append('./coeqwalpackage')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import re\n",
    "from coeqwalpackage.metrics import *\n",
    "import cqwlutils as cu\n",
    "import plotting as pu\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3b296-8880-480a-87db-e79770d5a78b",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98c1d1a-a48e-44d1-9d6d-1376f08f930f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CtrlFile = 'CalSim3DataExtractionInitFile_v4.xlsx'\n",
    "CtrlTab = 'Init'\n",
    "\n",
    "ScenarioListFile, ScenarioListTab, ScenarioListPath, DVDssNamesOutPath, SVDssNamesOutPath, ScenarioIndicesOutPath, DssDirsOutPath, VarListPath, VarListFile, VarListTab, VarOutPath, DataOutPath, ConvertDataOutPath, ExtractionSubPath, DemandDeliverySubPath, ModelSubPath, GroupDataDirPath, ScenarioDir, DVDssMin, DVDssMax, SVDssMin, SVDssMax, NameMin, NameMax, DirMin, DirMax, IndexMin, IndexMax, StartMin, StartMax, EndMin, EndMax, VarMin, VarMax, DemandFilePath, DemandFileName, DemandFileTab, DemMin, DemMax, InflowOutSubPath, InflowFilePath, InflowFileName, InflowFileTab, InflowMin, InflowMax = cu.read_init_file(CtrlFile, CtrlTab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a87bb4-4c93-4370-a01e-e224d38f0e74",
   "metadata": {},
   "source": [
    "### Read scenario indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab5db2b-995f-4187-bbcb-4243e0b157ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexhdr, index_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, IndexMin, IndexMax, hdr=True)\n",
    "index_names = []\n",
    "for i in range(len(index_name)):\n",
    "    index_names.append(index_name[i][0])\n",
    "index_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc643c20-11ce-4986-8252-05e0eb07a7ea",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28487c0-1156-44a7-aed1-d419791a9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, dss_names = read_in_df(ConvertDataOutPath,DVDssNamesOutPath)\n",
    "df = add_water_year_column(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1c2e2-16be-40a1-89d5-89ca084a037e",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014dbcef-a166-4c26-9190-ed8fdf234101",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_delta_vars = [\"EM_EC_MONTH\", \"JP_EC_MONTH\"]\n",
    "export_vars = [\"TRACYEC_MAX14DAY\", \"BANKSEC_MAX14DAY\"]\n",
    "indelta_thresholds={\"Top\": 2500, \"Mid\": 1600, \"Low\": 900}\n",
    "export_thresholds={\"Top\": 2500, \"Mid\": 1600, \"Low\": 900}\n",
    "indelta_station_list=[\"EM\", \"JP\"]\n",
    "export_station_list=[\"BANKSEC\", \"TRACYEC\"]\n",
    "indelta_rules = OrderedDict([\n",
    "    (1, {\"LT_A\": 0.75, \"LT_B\": None, \"GT_C\": 0.05}),\n",
    "    (2, {\"LT_A\": 0.65, \"LT_B\": 0.75, \"GT_C\": 0.12}),\n",
    "    (3, {\"LT_A\": 0.55, \"LT_B\": 0.65, \"GT_C\": 0.20}),\n",
    "])\n",
    "x2 = 'X2_PRV_KM'\n",
    "compliance_points_indelta = [\"EM_EC_MONTH\", \"JP_EC_MONTH\", \"RS_EC_MONTH\", \"CO_EC_MONTH\"]\n",
    "compliance_points_export = [\"BANKSEC\", \"TRACYEC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139d63c-5ed8-4e30-8b49-9270c59820bb",
   "metadata": {},
   "source": [
    "### Subset data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d59f7d-ad15-4341-bde6-76eef8f152d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_delta_df = create_subset_list(df, in_delta_vars)\n",
    "in_delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f732a-29b4-4cbb-b8d8-d5fab7e1148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = create_subset_list(df, export_vars)\n",
    "export_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e81cae-cbfb-4a1e-8d0b-695f0f13b024",
   "metadata": {},
   "source": [
    "### X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd88b09-65a4-4d90-9758-60737195870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annual Average\n",
    "april_x2_ann_avg = compute_annual_means(df, x2, units=\"KM\", months=[4])\n",
    "september_x2_ann_avg = compute_annual_means(df, x2, units=\"KM\", months=[9])\n",
    "\n",
    "# Annual CV\n",
    "april_x2_ann_cv = compute_cv(df, x2, \"April_X2_CV\", months=[4], units=\"KM\")\n",
    "april_x2_ann_cv.index.name = 'Scenario'\n",
    "september_x2_ann_cv = compute_cv(df, x2, \"September_X2_CV\", months=[9], units=\"KM\")\n",
    "september_x2_ann_cv.index.name = 'Scenario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b61b78-3630-441a-8d59-efd6c706a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up dataframes for export\n",
    "april_x2_ann_avg.columns = 'AprilX2_' + april_x2_ann_avg.columns.get_level_values('B').str.split('_').str[3]\n",
    "april_x2_ann_avg.columns.name = None\n",
    "\n",
    "september_x2_ann_avg.columns = 'SeptemberX2_' + september_x2_ann_avg.columns.get_level_values('B').str.split('_').str[3]\n",
    "september_x2_ann_avg.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ab69f-a6af-4067-b4bc-3e30fb32b3c0",
   "metadata": {},
   "source": [
    "### Salinity at compliance points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ffe3d-3350-4112-928f-77d4b60d95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "salinity_compliance_points_indelta_df = create_subset_list(df, compliance_points_indelta)\n",
    "salinity_compliance_points_export_df = create_subset_list(df, compliance_points_export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ede99-900a-462e-a7ca-e5c0b361e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up dataframes for export\n",
    "salinity_compliance_points_indelta_df.columns = salinity_compliance_points_indelta_df.columns.get_level_values('B')\n",
    "salinity_compliance_points_indelta_df.columns.name = None\n",
    "\n",
    "salinity_compliance_points_export_df.columns = salinity_compliance_points_export_df.columns.get_level_values('B')\n",
    "salinity_compliance_points_export_df.columns.name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec6d94-4d6b-41ac-b8ce-07a066701395",
   "metadata": {},
   "source": [
    "### Tier calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff718c-1277-4d5b-9ab9-f5bb87153107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_indelta_tier(\n",
    "    df,\n",
    "    scenID,\n",
    "    stations=[\"EM_EC_MONTH\", \"JP_EC_MONTH\"],\n",
    "    thresholds={\"Top\": 2500, \"Mid\": 1600, \"Low\": 900},\n",
    "    tier_rules=OrderedDict([\n",
    "        (1, {\"LT_A\": 0.75, \"LT_B\": None, \"GT_C\": 0.05}),\n",
    "        (2, {\"LT_A\": 0.65, \"LT_B\": 0.75, \"GT_C\": 0.12}),\n",
    "        (3, {\"LT_A\": 0.55, \"LT_B\": 0.65, \"GT_C\": 0.20}),\n",
    "    ])\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate in-delta tier designation for a given scenario.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with salinity variables.\n",
    "    scenID : str\n",
    "        Scenario identifier.\n",
    "    in_delta_vars : list of str, optional\n",
    "        Variables to include (default: [\"EM_EC_MONTH\", \"JP_EC_MONTH\"]).\n",
    "    thresholds : dict, optional\n",
    "        Thresholds for salinity (default: {\"Top\": 2500, \"Mid\": 1600, \"Low\": 900}).\n",
    "    tier_rules : dict, optional\n",
    "        Rules for assigning tiers. Each tier is an ordered dict with keys \"LT_A\", \"LT_B\", \"GT_C\".\n",
    "        Example (default):\n",
    "        ([\n",
    "            (1, {\"LT_A\": 0.75, \"LT_B\": None, \"GT_C\": 0.05}),\n",
    "            (2, {\"LT_A\": 0.65, \"LT_B\": 0.75, \"GT_C\": 0.12}),\n",
    "            (3, {\"LT_A\": 0.55, \"LT_B\": 0.65, \"GT_C\": 0.20}),\n",
    "        ])        \n",
    "    If no rule matches, returns tier = np.nan.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    idx = pd.IndexSlice\n",
    "\n",
    "    tA, tB, tC = thresholds[\"Low\"], thresholds[\"Mid\"], thresholds[\"Top\"]\n",
    "\n",
    "    # get the data for this scenario\n",
    "    selcols = [c for c in df.columns if scenID in c[1]]\n",
    "    if len(selcols) < len(in_delta_vars):\n",
    "        raise ValueError(f\"Didn't find the salinity columns for scenario {scenID}\")\n",
    "\n",
    "    thisdat = df.loc[:, selcols]\n",
    "\n",
    "    # store fractions for each variable\n",
    "    fracs = {}\n",
    "    for var in in_delta_vars:\n",
    "        col = idx[:, f\"{var}_{scenID}\"]\n",
    "        values = thisdat.loc[:, col].values\n",
    "\n",
    "        fracs[var] = {\n",
    "            \"LT_A\": sum(values < tA) / len(values),\n",
    "            \"LT_B\": sum(values < tB) / len(values),\n",
    "            \"LT_C\": sum(values < tC) / len(values),\n",
    "            \"GT_C\": sum(values > tC) / len(values),\n",
    "        }\n",
    "\n",
    "    # aggregate across vars\n",
    "    max_GT_C = max(v[\"GT_C\"] for v in fracs.values())\n",
    "    min_LT_A = min(v[\"LT_A\"] for v in fracs.values())\n",
    "    min_LT_B = min(v[\"LT_B\"] for v in fracs.values())\n",
    "\n",
    "    # apply tier rules in order\n",
    "    for tier, rule in tier_rules.items():\n",
    "        cond_A = min_LT_A >= rule[\"LT_A\"] if rule[\"LT_A\"] is not None else True\n",
    "        cond_B = min_LT_B >= rule[\"LT_B\"] if rule[\"LT_B\"] is not None else True\n",
    "        cond_C = max_GT_C < rule[\"GT_C\"] if rule[\"GT_C\"] is not None else True\n",
    "\n",
    "        if cond_A and cond_B and cond_C:\n",
    "            return tier\n",
    "\n",
    "    # default if no rule matches\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27775ea-6364-434d-a823-2f1c30fdd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_salinity_tier_assignment_matrix(\n",
    "    df,\n",
    "    station_list=[\"EM\", \"JP\"],\n",
    "    thresholds={\"Top\": 2500, \"Mid\": 1600, \"Low\": 900},\n",
    "    start_date=\"1921-10-01\"\n",
    "):\n",
    "    def extract_scenario_id(colname):\n",
    "        name = \"_\".join(colname) if isinstance(colname, tuple) else str(colname)\n",
    "        match = re.search(r's\\d{4}', name)\n",
    "        return match.group(0) if match else None\n",
    "\n",
    "    def extract_station_name(colname):\n",
    "        name = \"_\".join(colname) if isinstance(colname, tuple) else str(colname)\n",
    "        for st in station_list:\n",
    "            if name.startswith(st + \"_\") or f\"_{st}_\" in name:\n",
    "                return st\n",
    "        return None\n",
    "\n",
    "    def assign_tiers_by_scenario(df, date_series):\n",
    "        tier_rows = []\n",
    "        scenario_map = {}\n",
    "\n",
    "        for col in df.columns:\n",
    "            sid = extract_scenario_id(col)\n",
    "            station = extract_station_name(col)\n",
    "            if sid and station:\n",
    "                scenario_map.setdefault(sid, {})[station] = col\n",
    "\n",
    "        print(f\"Found {len(scenario_map)} scenarios: {list(scenario_map.keys())}\")\n",
    "\n",
    "        for sid, col_dict in scenario_map.items():\n",
    "            if not all(st in col_dict for st in station_list):\n",
    "                print(f\" Skipping {sid}: missing one or more station columns\")\n",
    "                continue\n",
    "\n",
    "            df_scenario = pd.DataFrame(\n",
    "                {st: df[col_dict[st]] for st in station_list},\n",
    "                index=date_series\n",
    "            )\n",
    "            df_scenario[\"Year\"] = df_scenario.index.year\n",
    "\n",
    "            valid_rows = df_scenario.dropna(subset=station_list)\n",
    "            if valid_rows.empty:\n",
    "                print(f\" Skipping {sid}: all data is NaN\")\n",
    "                continue\n",
    "\n",
    "            yearly = valid_rows.groupby(\"Year\")\n",
    "            valid_years = list(yearly.groups.keys())\n",
    "            total_years = len(valid_years)\n",
    "\n",
    "            tier4_flag = False\n",
    "            tier3_flag = False\n",
    "            tier3_years_with_1month_over_mid = 0\n",
    "            tier2_valid_years = 0\n",
    "            tier1_valid_years = 0\n",
    "            any_year_exceeds_mid = False\n",
    "\n",
    "            for year, group in yearly:\n",
    "                readings = {st: group[st] for st in station_list}\n",
    "\n",
    "                if any((r > thresholds[\"Top\"]).sum() >= 2 for r in readings.values()):\n",
    "                    tier4_flag = True\n",
    "                    break\n",
    "\n",
    "                if any((r > thresholds[\"Mid\"]).sum() >= 2 for r in readings.values()):\n",
    "                    tier3_flag = True\n",
    "\n",
    "                if any((r > thresholds[\"Mid\"]).any() for r in readings.values()):\n",
    "                    tier3_years_with_1month_over_mid += 1\n",
    "\n",
    "\n",
    "                if any((r > thresholds[\"Mid\"]).any() for r in readings.values()):\n",
    "                    any_year_exceeds_mid = True\n",
    "                else:\n",
    "                    in_range_counts = [((r >= thresholds[\"Low\"]) & (r <= thresholds[\"Mid\"])).sum() for r in readings.values()]\n",
    "                    if all(count >= 10 for count in in_range_counts):\n",
    "                        tier2_valid_years += 1\n",
    "\n",
    "\n",
    "                if all(((r < thresholds[\"Low\"]).sum() == 12) for r in readings.values()):\n",
    "                    tier1_valid_years += 1\n",
    "\n",
    "            if total_years == 0:\n",
    "                print(f\" Scenario {sid}: No valid years with complete data.\")\n",
    "                continue\n",
    "\n",
    "            if tier4_flag:\n",
    "                tier = 4\n",
    "            elif tier3_flag or (tier3_years_with_1month_over_mid / total_years > 0.05):\n",
    "                tier = 3\n",
    "            elif not any_year_exceeds_mid and (tier2_valid_years / total_years >= 0.95):\n",
    "                tier = 2\n",
    "            elif tier1_valid_years / total_years >= 0.95:\n",
    "                tier = 1\n",
    "            else:\n",
    "                tier = None\n",
    "                print(f\" Scenario {sid} did not match any tier.\")\n",
    "                print(f\"   Summary: tier3_flag={tier3_flag}, tier3_pct={tier3_years_with_1month_over_mid / total_years:.2f}, \"\n",
    "                      f\"tier2_pct={tier2_valid_years / total_years:.2f}, tier1_pct={tier1_valid_years / total_years:.2f}, \"\n",
    "                      f\"any_year_exceeds_mid={any_year_exceeds_mid}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"→ Scenario {sid} assigned Tier {tier}\")\n",
    "            tier_rows.append({\n",
    "                \"Scenario\": sid,\n",
    "                \"Salinity_Tier\": tier\n",
    "            })\n",
    "\n",
    "        return pd.DataFrame(tier_rows, columns=[\"Scenario\", \"Salinity_Tier\"])\n",
    "\n",
    "    df = df.copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
    "        df.index = pd.date_range(start=start_date, periods=len(df), freq=\"MS\")\n",
    "\n",
    "    date_series = df.index\n",
    "    tier_df = assign_tiers_by_scenario(df, date_series)\n",
    "\n",
    "    if tier_df.empty:\n",
    "        print(\" No valid scenario-station pairs were found.\")\n",
    "        return pd.DataFrame(columns=[\"Salinity_Tier\"])\n",
    "\n",
    "    return tier_df.set_index(\"Scenario\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50348474-478a-40d2-9efc-912fdd7c27c6",
   "metadata": {},
   "source": [
    "### In-Delta tier assignment (new version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeffde9-fd13-4fb0-b453-a82542ae10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiers = {} #<- dictionary to store results\n",
    "for scenID in index_names: # iterate through the list of scenario IDs\n",
    "    # call the function defined above\n",
    "    tiers[scenID] = calc_indelta_tier(df = in_delta_df, scenID = scenID, stations = in_delta_vars, thresholds =  indelta_thresholds, tier_rules= indelta_rules)\n",
    "    # print out the progress\n",
    "    print(f\"assigned tier {tiers[scenID]} to scenario {scenID}\")\n",
    "\n",
    "# create a dataframe from the dictionary - index is the scenario id, \n",
    "# single column is the tier value\n",
    "tier_indelta_df = pd.DataFrame.from_dict(tiers, orient='index', columns=['Salinity_Tier']) #, index='ScenarioID')\n",
    "tier_indelta_df.index.name = 'ScenarioID' #rename the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b86bb-bda3-407d-bcf6-185cea9699ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_indelta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fee60-ff33-4d88-9970-bfc4547b3235",
   "metadata": {},
   "source": [
    "### In-Delta tier assignment (old version):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d39682-083f-4fde-8f9b-0b512d3b3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tier_indeltaold_df = generate_salinity_tier_assignment_matrix(\n",
    "#     df=in_delta_df,\n",
    "#     station_list=indelta_station_list,\n",
    "#     thresholds=indelta_thresholds,\n",
    "#     start_date=\"1921-10-01\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35167f78-b855-450f-8830-9b630e521e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tier_indeltaold_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451ff23e-3d39-4319-8720-f8a8b5d86cca",
   "metadata": {},
   "source": [
    "### Export tier assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb01c6f-08dc-437b-ba5c-a36836d238f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_export_df = generate_salinity_tier_assignment_matrix(\n",
    "    df=export_df,\n",
    "    station_list=export_station_list,\n",
    "    thresholds=export_thresholds,\n",
    "    start_date=\"1921-10-01\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19e227e-710a-41f4-8f86-d97135f1b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_export_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1510a1-f1d5-4c48-af91-e228d9de3e80",
   "metadata": {},
   "source": [
    "### Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16223caa-c79a-4020-ad59-50e80b9321fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_calsim_base_path(start_path, target_folder=\"CalSim3_Model_Runs\"):\n",
    "    # Go up twice to reach DSP, then look for sibling folder\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    dsp_root = os.path.dirname(os.path.dirname(current_path))  # notebooks → coeqwal → DSP\n",
    "    candidate = os.path.join(dsp_root, target_folder)\n",
    "    if os.path.isdir(candidate):\n",
    "        return candidate\n",
    "    raise FileNotFoundError(f\"{target_folder} not found alongside {dsp_root}\")\n",
    "\n",
    "\n",
    "base_dir = os.path.abspath(\".\")\n",
    "calsim_base_path = find_calsim_base_path(base_dir)\n",
    "\n",
    "salinity_output_dir = os.path.join(\n",
    "    calsim_base_path,\n",
    "    \"Scenarios\",\n",
    "    \"Performance_Metrics\",\n",
    "    \"Tiered_Outcome_Measures\",\n",
    "    \"Salinity\"\n",
    ")\n",
    "os.makedirs(salinity_output_dir, exist_ok=True)\n",
    "\n",
    "april_x2_ann_avg_path = os.path.join(salinity_output_dir, \"AprilX2_AnnualAverage.csv\")\n",
    "april_x2_ann_avg.to_csv(april_x2_ann_avg_path, index=True)\n",
    "\n",
    "april_x2_ann_cv_path = os.path.join(salinity_output_dir, \"AprilX2_AnnualCV.csv\")\n",
    "april_x2_ann_cv.to_csv(april_x2_ann_cv_path, index=True)\n",
    "\n",
    "september_x2_ann_avg_path = os.path.join(salinity_output_dir, \"SeptemberX2_AnnualAverage.csv\")\n",
    "september_x2_ann_avg.to_csv(september_x2_ann_avg_path, index=True)\n",
    "\n",
    "september_x2_ann_cv_path = os.path.join(salinity_output_dir, \"SeptemberX2_AnnualCV.csv\")\n",
    "september_x2_ann_cv.to_csv(september_x2_ann_cv_path, index=True)\n",
    "\n",
    "salinity_compliance_points_indelta_path = os.path.join(salinity_output_dir, \"InDeltaSalinity.csv\")\n",
    "salinity_compliance_points_indelta_df.to_csv(salinity_compliance_points_indelta_path, index=True)\n",
    "\n",
    "salinity_compliance_points_export_path = os.path.join(salinity_output_dir, \"ExportSalinity.csv\")\n",
    "salinity_compliance_points_export_df.to_csv(salinity_compliance_points_export_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab1fd2-63c6-4663-bd46-3bfea904fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "salinity_output_path = os.path.join(salinity_output_dir, \"InDeltaTierAssignment.csv\")\n",
    "tier_indelta_df.to_csv(salinity_output_path, index=True)\n",
    "\n",
    "salinity_output_path = os.path.join(salinity_output_dir, \"ExportTierAssignment.csv\")\n",
    "tier_export_df.to_csv(salinity_output_path, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d94fc-15dc-49c3-a7e6-6fcea193b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(salinity_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3824a6a3-6045-4541-a541-88379b44f878",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916fc0fd-f6d7-40f8-a4cf-a5d1d2d0b3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6caf8-d347-451a-8748-10a646074c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
