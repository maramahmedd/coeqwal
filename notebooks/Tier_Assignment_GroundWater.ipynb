{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a9bc43-c6c3-4103-a45d-55b7a2938c13",
   "metadata": {},
   "source": [
    "## Flags and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e5d6c-ee9e-460a-86e7-c5f1ed53c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "SaveTrendPlots = True\n",
    "SaveTrendComparisonsPlots = True\n",
    "SaveTierMaps = True\n",
    "SaveSlopeMap = True\n",
    "SaveHistogram = True\n",
    "\n",
    "YearsToAverage = 10 # years to average when computing end-start diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a2de5-6061-4a09-a60b-b6616181ce82",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f595120-8cfd-49e4-92c4-bc38bf5f8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a8d63-e76d-401e-ba1f-7f7b04c1b946",
   "metadata": {},
   "source": [
    "## Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c91bd1a1e8beca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T08:04:38.276992Z",
     "start_time": "2024-07-20T08:04:38.274280Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# append coeqwal packages to path\n",
    "sys.path.append('./coeqwalpackage')\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cqwlutils as cu\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9145a-2421-4dd1-a31d-1edb2b5a4f64",
   "metadata": {},
   "source": [
    "## Import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61561cb3284fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T20:35:57.550559300Z",
     "start_time": "2024-03-07T20:35:57.462942100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import custom modules - NEED WINDOWS OS (NOTE: I had to run this twice, must check why this happens!)\n",
    "from coeqwalpackage.DataExtraction import *\n",
    "from coeqwalpackage.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfeecf5-b49d-4ff4-8b7e-4867879ce50c",
   "metadata": {},
   "source": [
    "# Specify baseline scenario and other constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c124bbc8-1215-43a8-ace0-3c49f7153317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "baseline_scenario = \"s0002\"\n",
    "end_year_override = {\"s0006\", \"s0007\", \"s0008\", \"s0009\", \"s0010\"} #scenarios that end before others\n",
    "drop_threshold = 1000\n",
    "start_year = 1960\n",
    "start_date = pd.Timestamp(\"1990-01-01\")\n",
    "end_date = pd.Timestamp(\"2000-12-31\")\n",
    "slope_start_date=\"1992-09-30\"\n",
    "window_start = pd.Timestamp(\"1973-10-31\") # GW simulation start and end\n",
    "window_end   = pd.Timestamp(\"2015-09-30\")\n",
    "monthly_ft_filename = \"GroundWater_Levels_Monthly.csv\"\n",
    "annual_ft_filename = \"GroundWater_Levels_Annual.csv\"\n",
    "monthly_af_filename = \"GroundWater_Volumes_Monthly.csv\"\n",
    "annual_af_filename = \"GroundWater_Volumes_Annual.csv\"\n",
    "monthly_volper_filename = \"GroundWater_Levels_BaselinePercent_Monthly.csv\"\n",
    "annual_volper_filename = \"GroundWater_Levels_BaselinePercent_Annual.csv\"\n",
    "monthly_levper_filename = \"GroundWater_Levels_BaselinePercent_Monthly.csv\"\n",
    "annual_levper_filename = \"GroundWater_Levels_BaselinePercent_Annual.csv\"\n",
    "monthly_volpercv_filename = \"GroundWater_Levels_BaselinePercentCV_Monthly.csv\"\n",
    "annual_volpercv_filename = \"GroundWater_Levels_BaselinePercentCV_Annual.csv\"\n",
    "monthly_levpercv_filename = \"GroundWater_Levels_BaselinePercentCV_Monthly.csv\"\n",
    "annual_levpercv_filename = \"GroundWater_Levels_BaselinePercentCV_Annual.csv\"\n",
    "monthly_ft_cv_filename = \"GroundWater_Levels_CV_monthly.csv\"\n",
    "annual_ft_cv_filename = \"GroundWater_Levels_CV_annual.csv\"\n",
    "monthly_af_cv_filename = \"GroundWater_Volumes_CV_monthly.csv\"\n",
    "annual_af_cv_filename = \"GroundWater_Volumes_CV_annual.csv\"\n",
    "trend_filename = \"GroundWater_Trends_ft_per_month.csv\"\n",
    "diff_filename = \"GroundWater_AvgEndStartDiff_ft.csv\"\n",
    "GWregionIndex_filename = \"CalSim3GWregionIndex.wresl\"\n",
    "WBAIndex_filename = \"CalSim3_WBA.csv\"\n",
    "WBAStorage_filename = \"20250908draft_C2VSim_73-15_WBA_Storage.csv\"\n",
    "GWwreslmapping_filename = \"groundwater_wresl_mapping.csv\"\n",
    "tier_filename = \"GroundWater_Tiers.csv\"\n",
    "shapefile_filename = \"./shapefiles (1)/i12_CalSim3Model_WaterBudgetAreas_20221021.shp\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e114a-18ec-47a1-bbdb-6911c35721a0",
   "metadata": {},
   "source": [
    "## Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef5196-3105-46dd-b2d7-3261dfcb1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "def find_repo_root(marker_dirs=(\"CalSim3_Model_Runs\", \"coeqwal\")) -> Path:\n",
    "    \"\"\"\n",
    "    Walk upward from the script (or notebook CWD) until we find a directory\n",
    "    that contains the expected project folders (e.g., 'CalSim3_Model_Runs' and 'coeqwal').\n",
    "    \"\"\"\n",
    "    # 1) Optional env override\n",
    "    env_root = os.environ.get(\"DSP_REPO_ROOT\")\n",
    "    if env_root:\n",
    "        root = Path(env_root).expanduser().resolve()\n",
    "        if all((root / m).exists() for m in marker_dirs):\n",
    "            return root\n",
    "\n",
    "    # 2) Start from script dir if available; else notebook working dir\n",
    "    try:\n",
    "        start = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        start = Path.cwd().resolve()\n",
    "\n",
    "    # 3) Walk up until markers are found\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if all((parent / m).exists() for m in marker_dirs):\n",
    "            return parent\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate repo root containing: \"\n",
    "        + \", \".join(marker_dirs)\n",
    "        + \". Set DSP_REPO_ROOT env var or run from inside the repo.\"\n",
    "    )\n",
    "\n",
    "repo_root = find_repo_root()\n",
    "\n",
    "output_dir = repo_root / \"CalSim3_Model_Runs\" / \"Scenarios\" / \"Performance_Metrics\" / \"Tiered_Outcome_Measures\" / \"Groundwater\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "trends_output_dir = repo_root / \"CalSim3_Model_Runs\" / \"Scenarios\" / \"Performance_Metrics\" / \"Tiered_Outcome_Measures\" / \"Groundwater\" / \"TrendPlots\"\n",
    "trends_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "trends_comparisons_output_dir = repo_root / \"CalSim3_Model_Runs\" / \"Scenarios\" / \"Performance_Metrics\" / \"Tiered_Outcome_Measures\" / \"Groundwater\" / \"TrendComparisonPlots\"\n",
    "trends_comparisons_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_output_dir = repo_root / \"CalSim3_Model_Runs\" / \"Scenarios\" / \"Performance_Metrics\" / \"Tiered_Outcome_Measures\" / \"Groundwater\" / \"Data\"\n",
    "data_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "tiers_output_dir = repo_root / \"CalSim3_Model_Runs\" / \"Scenarios\" / \"Performance_Metrics\" / \"Tiered_Outcome_Measures\" / \"Groundwater\" / \"TierMaps\"\n",
    "tiers_output_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb543aa5-b5ba-4313-a89e-12bade27e382",
   "metadata": {},
   "source": [
    "## Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc08377-e723-4426-8e0c-8b56dd8cb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(\".\")\n",
    "wba_storage_csv_path = os.path.join(base_dir, WBAStorage_filename)\n",
    "output_mappingcsv_path = os.path.join(base_dir, GWwreslmapping_filename)\n",
    "wresl_path = os.path.join(base_dir, GWregionIndex_filename)\n",
    "wba_csv_path = os.path.join(base_dir, WBAIndex_filename)\n",
    "tier_output_path = os.path.join(data_output_dir, tier_filename)\n",
    "shapefile_path = os.path.join(base_dir, shapefile_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a886dad8c6c902",
   "metadata": {},
   "source": [
    "## Define contol file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0f49b-17c9-40f5-8861-5680673a5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "CtrlFile = 'CalSim3GroundWaterDataExtractionInitFile_v1.xlsx'\n",
    "CtrlTab = 'Init'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070e30e-fec8-4e22-bf9f-bd43e1d4f7a9",
   "metadata": {},
   "source": [
    "## Read from control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614d95e-3b73-42f9-a330-7750b45b921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScenarioListFile, ScenarioListTab, ScenarioListPath, GW1DssNamesOutPath, GW2DssNamesOutPath, ScenarioIndicesOutPath, DssDirsOutPath, VarListPath, VarListFile, VarListTab, VarOutPath, DataOutPath, ConvertDataOutPath, ExtractionSubPath, DemandDeliverySubPath, ModelSubPath, GroupDataDirPath, ScenarioDir, GW1DssMin, GW1DssMax, GW2DssMin, GW2DssMax, NameMin, NameMax, DirMin, DirMax, IndexMin, IndexMax, StartMin, StartMax, EndMin, EndMax, VarMin, VarMax, DemandFilePath, DemandFileName, DemandFileTab, DemMin, DemMax, InflowOutSubPath, InflowFilePath, InflowFileName, InflowFileTab, InflowMin, InflowMax = cu.read_init_file(CtrlFile, CtrlTab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2529c38-c663-43ee-811d-415fdd815743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ScenarioListFile, ScenarioListTab, ScenarioListPath, GW1DssNamesOutPath, GW2DssNamesOutPath, ScenarioIndicesOutPath, DssDirsOutPath, VarListPath, VarListFile, VarListTab, VarOutPath, DataOutPath, ConvertDataOutPath, ExtractionSubPath, DemandDeliverySubPath, ModelSubPath, GroupDataDirPath, ScenarioDir, GW1DssMin, GW1DssMax, GW2DssMin, GW2DssMax, NameMin, NameMax, DirMin, DirMax, IndexMin, IndexMax, StartMin, StartMax, EndMin, EndMax, VarMin, VarMax, DemandFilePath, DemandFileName, DemandFileTab, DemMin, DemMax, InflowOutSubPath, InflowFilePath, InflowFileName, InflowFileTab, InflowMin, InflowMax])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd54452-9c03-4f71-ba08-06fe3b8ba071",
   "metadata": {},
   "source": [
    "## Check for output directory and create if necessary (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bb493-d379-4c25-a56e-e3da4fab4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if output directory exists\n",
    "if not os.path.exists(GroupDataDirPath):\n",
    "    # print warning\n",
    "    print(\"Warning: directory \" + GroupDataDirPath + \" does not exists and will be created\")\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(GroupDataDirPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f736a-622a-4ca0-86f7-8979963700bf",
   "metadata": {},
   "source": [
    "## Define Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17492ae9-5a01-4eb2-b6e1-4422fbd39d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN values as defined by CalSim3\n",
    "Nan1 = -901\n",
    "Nan2 = -902"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf686-69bd-4a22-bc35-42769740aae7",
   "metadata": {},
   "source": [
    "## Read indeces, dss names, directory names, start and end dates, time range (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2a998-acab-49e7-a2bd-25e6336f0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw1dsshdr, gw1dssname = cu.read_from_excel(ScenarioListPath, ScenarioListTab, GW1DssMin, GW1DssMax, hdr=True)\n",
    "gw1dss_names = []\n",
    "for i in range(len(gw1dssname)):\n",
    "    gw1dss_names.append(gw1dssname[i][0])\n",
    "gw1dss_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511228c-0d07-4308-86e7-bc1f9a3f74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw2dsshdr, gw2dssname = cu.read_from_excel(ScenarioListPath, ScenarioListTab, GW2DssMin, GW2DssMax, hdr=True)\n",
    "gw2dss_names = []\n",
    "for i in range(len(gw2dssname)):\n",
    "    gw2dss_names.append(gw2dssname[i][0])\n",
    "gw2dss_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903558b7-1a87-4911-ba9a-2f7ac847da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexhdr, index_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, IndexMin, IndexMax, hdr=True)\n",
    "index_names = []\n",
    "for i in range(len(index_name)):\n",
    "    index_names.append(index_name[i][0])\n",
    "index_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26379-84b3-425a-a65e-2b6e6852e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyhdr, study_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, NameMin, NameMax, hdr=True)\n",
    "study_names = []\n",
    "for i in range(len(study_name)):\n",
    "    study_names.append(study_name[i][0])\n",
    "study_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0c016-d82e-4927-b944-f09db7b2f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirhdr, dir_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, DirMin, DirMax, hdr=True)\n",
    "dir_names = []\n",
    "for i in range(len(dir_name)):\n",
    "    dir_names.append(dir_name[i][0])\n",
    "dir_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456c0e6-61d8-427b-8d45-1814053c4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "starthdr, start_date = cu.read_from_excel(ScenarioListPath, ScenarioListTab, StartMin, StartMax, hdr=True)\n",
    "start_dates = []\n",
    "for i in range(len(start_date)):\n",
    "    start_dates.append(start_date[i][0])\n",
    "datetime_start_dates = pd.to_datetime(start_dates)\n",
    "# turns out that dss reading library wands a dt datetime, not pd datetime\n",
    "dt_datetime_start_dates = [dt.to_pydatetime() for dt in datetime_start_dates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a887b-829e-4773-bb8c-60e8d6ff35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "endhdr, end_date = cu.read_from_excel(ScenarioListPath, ScenarioListTab, EndMin, EndMax, hdr=True)\n",
    "end_dates = []\n",
    "for i in range(len(end_date)):\n",
    "    end_dates.append(end_date[i][0])\n",
    "# turns out that dss reading library wands a dt datetime, not pd datetime\n",
    "datetime_end_dates = pd.to_datetime(end_dates)\n",
    "dt_datetime_end_dates = [dt.to_pydatetime() for dt in datetime_end_dates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b72d5a-0f63-4a97-958b-00ada84e768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_datetime = min(dt_datetime_start_dates)\n",
    "print('Min time: ')\n",
    "print(min_datetime)\n",
    "max_datetime = max(dt_datetime_end_dates)\n",
    "print('Max time: ')\n",
    "print(max_datetime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19277cc0585f1d94",
   "metadata": {},
   "source": [
    "## Read variables list (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3f15cd87dd616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T01:54:08.182311Z",
     "start_time": "2024-04-09T01:54:08.163835Z"
    }
   },
   "outputs": [],
   "source": [
    "# get vars\n",
    "hdr, vars = cu.read_from_excel(VarListPath, VarListTab,VarMin,VarMax,hdr=True)\n",
    "gw1var_df = pd.DataFrame(data=vars, columns=hdr)\n",
    "gw1var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abcc23-c788-42b6-948f-a03361901e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gw1var_df.head(20))   # show first 20 rows\n",
    "print(gw1var_df.columns)    # see what metadata is included\n",
    "print(gw1var_df.shape)      # rows × cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a21684-0557-41e0-b4d2-5ea587fafd5d",
   "metadata": {},
   "source": [
    "## Read the compund data from CSV to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ed99e-35fb-4008-8692-a032f60e8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe from CSV\n",
    "print('Reading ' + DataOutPath)\n",
    "gw1_df, gw1dss_names = read_in_df(DataOutPath,GW1DssNamesOutPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987e457-e886-41f8-a3ae-8261d1e651bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gw1dss_names:\")\n",
    "gw1dss_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acba19-3ddf-4622-9d15-321557ec14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gw1_df:\")\n",
    "gw1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae331c-eeb9-47ad-93a1-b8c6d2b5f7c0",
   "metadata": {},
   "source": [
    "## Drop the LT:E999 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbcf99-2ce7-4b04-aa0a-3264fa5674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~gw1_df.columns.to_frame().apply(lambda col: col.astype(str).str.contains('LT:E999')).any(axis=1)\n",
    "gw1_df = gw1_df.loc[:, mask.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764c707-1103-4901-ba5d-49a7a9c63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"new gw1_df:\")\n",
    "gw1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fea1da-0d20-48f0-a906-8a7e61397676",
   "metadata": {},
   "source": [
    "## Add water year column to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7d4ee-c312-4a0f-a89f-113641fb5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_water_year_column(df):\n",
    "    df_copy = df.copy().sort_index()\n",
    "    df_copy['Date'] = pd.to_datetime(df_copy.index)\n",
    "    df_copy.loc[:, 'Year'] = df_copy['Date'].dt.year\n",
    "    df_copy.loc[:, 'Month'] = df_copy['Date'].dt.month\n",
    "    df_copy.loc[:, 'WaterYear'] = np.where(df_copy['Month'] >= 10, df_copy['Year'] + 1, df_copy['Year'])\n",
    "    return df_copy.drop([\"Date\", \"Year\", \"Month\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa138763-4196-4fe7-8c48-f97f5e277d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw1_df = add_water_year_column(gw1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723b11a-2446-437c-8d5c-060fbfb37d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gw1_df with water year column:\")\n",
    "gw1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a12804-a4b2-4906-b621-fee26703b776",
   "metadata": {},
   "source": [
    "## End of initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd61217-e50d-4622-982b-bc34e5499eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done Initializing!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95274915-8723-4d8f-a2cb-2174804d575a",
   "metadata": {},
   "source": [
    "## Map SR to WBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ae3ad-5839-43cf-a557-638d77f26a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "# Read the area data (CSV)\n",
    "wba_df = pd.read_csv(wba_csv_path)\n",
    "\n",
    "# Check what columns are present\n",
    "print(\"Columns in WBA Area CSV:\", wba_df.columns)\n",
    "\n",
    "# Preview key columns\n",
    "print(wba_df[['fid', 'GIS_Acres']].head())\n",
    "\n",
    "# === Step 3: Parse WRESL File to Map SRxx to WBAxx ===\n",
    "\n",
    "with open(wresl_path, 'r') as f:\n",
    "    wresl_lines = f.readlines()\n",
    "\n",
    "# Extract SRxx → WBAxx or DETAW\n",
    "sr_to_wba_map = {}\n",
    "for line in wresl_lines:\n",
    "    # Match standard WBA format: indxWBA_XX = SRYY\n",
    "    match = re.match(r'\\s*indxWBA_(\\d+)\\s*=\\s*(SR\\d+)', line)\n",
    "    if match:\n",
    "        wba_num, sr_num = match.groups()\n",
    "        sr_to_wba_map[sr_num] = f'WBA{wba_num}'\n",
    "    else:\n",
    "        # Match DETAW format: indxDETAW = SRYY\n",
    "        match_detaw = re.match(r'\\s*indxDETAW\\s*=\\s*(SR\\d+)', line)\n",
    "        if match_detaw:\n",
    "            sr_num = match_detaw.group(1)\n",
    "            sr_to_wba_map[sr_num] = 'DETAW'\n",
    "\n",
    "# # Convert the mapping to a DataFrame for easier viewing\n",
    "# mapping_df = pd.DataFrame(list(sr_to_wba_map.items()), columns=['SR_number', 'WBA_name'])\n",
    "\n",
    "# # Preview result\n",
    "# print(\"\\n=== SR to WBA Mapping Preview ===\")\n",
    "# print(mapping_df.head())\n",
    "\n",
    "# Save mapping to CSV if needed\n",
    "# mapping_df.to_csv(\"sr_to_wba_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd86ff6-2b57-40fc-aa8c-c7b2c01553bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wresl_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse mappings\n",
    "sr_to_wba_map = {}\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "\n",
    "    # Handle standard: define indxWBA_2 {value 1 }\n",
    "    match_wba = re.match(r'define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    if match_wba:\n",
    "        wba_id, sr_num = match_wba.groups()\n",
    "        sr_key = f\"SR{int(sr_num):02d}\"       # e.g. 1 → SR01\n",
    "        wba_value = f\"WBA{wba_id}\"            # e.g. 2 → WBA2\n",
    "        sr_to_wba_map[sr_key] = wba_value\n",
    "        continue\n",
    "\n",
    "    # Handle special case: define indxDETAW {value 42 }\n",
    "    match_detaw = re.match(r'define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    if match_detaw:\n",
    "        sr_num = match_detaw.group(1)\n",
    "        sr_key = f\"SR{int(sr_num):02d}\"       # e.g. 42 → SR42\n",
    "        sr_to_wba_map[sr_key] = \"DETAW\"\n",
    "\n",
    "# Convert to DataFrame\n",
    "mapping_df = pd.DataFrame(list(sr_to_wba_map.items()), columns=[\"SR_number\", \"WBA_name\"])\n",
    "print(mapping_df.head(10))  # check the DETAW row\n",
    "\n",
    "mapping_df.to_csv(\"sr_to_wba_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedad83-c48d-42b7-aa85-967e6e2c7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(wresl_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "mapping_records = []\n",
    "\n",
    "i = 1\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    # print(\"line \" + str(i) + \":\")\n",
    "    # print(line)\n",
    "    match_wba = re.match(r'define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    # print(\"match_wba \" + str(i) + \":\")\n",
    "    # print(match_wba)\n",
    "    \n",
    "    if match_wba:\n",
    "        wba_id, sr_num = match_wba.groups()\n",
    "        # print(\"wba_id \" + str(i) + \":\")\n",
    "        # print(wba_id)\n",
    "        # print(\"sr_num \" + str(i) + \":\")\n",
    "        # print(sr_num)\n",
    "        mapping_records.append({\n",
    "            \"Subregion_ID\": f\"SR{int(sr_num):02d}\",\n",
    "            \"WBA_ID\": f\"WBA{wba_id}\"\n",
    "        })\n",
    "        continue\n",
    "    i = i + 1\n",
    "    match_detaw = re.match(r'define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    if match_detaw:\n",
    "        sr_num = match_detaw.group(1)\n",
    "        mapping_records.append({\n",
    "            \"Subregion_ID\": f\"SR{int(sr_num):02d}\",\n",
    "            \"WBA_ID\": \"DETAW\"\n",
    "        })\n",
    "\n",
    "# print(\"mapping_records:\")\n",
    "# print(mapping_records)\n",
    "\n",
    "wresl_df = pd.DataFrame(mapping_records).sort_values(\"Subregion_ID\")\n",
    "display(wresl_df)\n",
    "\n",
    "wresl_df.to_csv(output_mappingcsv_path, index=False)\n",
    "print(f\"Saved WRESL mapping to: {output_mappingcsv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a35f52-c070-411d-a86d-9f017a6b7f8f",
   "metadata": {},
   "source": [
    "## Monthly, Annual Data and Trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579bec28-415e-4376-a3b5-478e582b29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "INPUT_GW_CSV = DataOutPath\n",
    "STORAGE_FILE = wba_storage_csv_path\n",
    "WBA_FILE     = wba_csv_path\n",
    "\n",
    "# Time window (C2VSim range)\n",
    "WINDOW_START = window_start\n",
    "WINDOW_END   = window_end\n",
    "START_YEAR_PRECLIP = start_year\n",
    "\n",
    "# =======================\n",
    "# DETAW area handling\n",
    "# =======================\n",
    "# If you know the exact DETAW area (in acres), set it here (e.g., 123456.0).\n",
    "DETAW_AREA_ACRES: Optional[float] = None\n",
    "# If the above is None, we will use the sum of all WBA GIS_Acres as the area for DETAW conversion.\n",
    "USE_TOTAL_WBA_ACRES_FOR_DETAW = True\n",
    "\n",
    "# =======================\n",
    "# Helpers\n",
    "# =======================\n",
    "def load_gw1_df(csv_path: Path) -> pd.DataFrame:\n",
    "    na_vals = [\"\", \"NA\", \"NaN\", \"nan\", \"-\", \"--\"]\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            csv_path,\n",
    "            header=[0,1,2,3,4],\n",
    "            index_col=0,\n",
    "            parse_dates=True,\n",
    "            low_memory=False,\n",
    "            na_values=na_vals\n",
    "        )\n",
    "        return df\n",
    "    except Exception:\n",
    "        df = pd.read_csv(csv_path, index_col=0, parse_dates=True, na_values=na_vals)\n",
    "        if len(df.columns) and isinstance(df.columns[0], str) and \"|\" in df.columns[0]:\n",
    "            tuples = [tuple(str(c).split(\"|\")) for c in df.columns]\n",
    "            if all(len(t) == 5 for t in tuples):\n",
    "                df.columns = pd.MultiIndex.from_tuples(\n",
    "                    tuples, names=[\"Model\", \"VarTag\", \"Type\", \"Timestep\", \"Unit\"]\n",
    "                )\n",
    "        return df\n",
    "\n",
    "def normalize_id(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.strip().upper()\n",
    "    if s.startswith(\"WBA\"):\n",
    "        s = s[3:]\n",
    "    return s.zfill(2) if s.isdigit() else s\n",
    "\n",
    "def normalize_storage_col(col: str) -> Optional[str]:\n",
    "    if not isinstance(col, str):\n",
    "        return None\n",
    "    if col.upper().startswith(\"WBA\"):\n",
    "        s = col.replace(\"WBA\", \"\", 1)\n",
    "        for suffix in (\"_STORAGE_AF\", \"_Storage_AF\", \"_storage_af\"):\n",
    "            if s.endswith(suffix):\n",
    "                s = s[:-len(suffix)]\n",
    "                break\n",
    "        s = s.strip().upper()\n",
    "        return s.zfill(2) if s.isdigit() else s\n",
    "    elif col.upper().startswith(\"DETAW\"):\n",
    "        return \"DETAW\"\n",
    "    return None\n",
    "\n",
    "def compute_cv_df(df):\n",
    "    \"\"\"\n",
    "    Compute coefficient of variation (std/mean) for each WBA_s#### column,\n",
    "    and return a DataFrame with scenarios as rows and WBA IDs as columns.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if \"_s\" not in col:\n",
    "            continue\n",
    "        wba_id, scen_raw = col.split(\"_s\", 1)\n",
    "        scenario = f\"s{scen_raw}\"\n",
    "\n",
    "        series = df[col].dropna()\n",
    "        if len(series) == 0:\n",
    "            continue\n",
    "\n",
    "        mean_val = series.mean()\n",
    "        std_val = series.std()\n",
    "        cv = std_val / mean_val if mean_val != 0 else np.nan\n",
    "\n",
    "        records.append({\"scenario\": scenario, \"WBA\": wba_id, \"CV\": cv})\n",
    "\n",
    "    cv_df = (\n",
    "        pd.DataFrame(records)\n",
    "        .pivot(index=\"scenario\", columns=\"WBA\", values=\"CV\")\n",
    "        .sort_index(axis=1)\n",
    "        .sort_index(axis=0)\n",
    "    )\n",
    "\n",
    "    return cv_df\n",
    "\n",
    "# --- Helper: normalize WBA name by stripping leading zeros in the numeric prefix ---\n",
    "def normalize_wba_name(wba: str) -> str:\n",
    "    if not isinstance(wba, str):\n",
    "        return str(wba)\n",
    "    s = wba.strip().upper()\n",
    "    # Keep non-WBA names as-is (e.g., DETAW)\n",
    "    if not s.startswith(\"WBA\"):\n",
    "        return s\n",
    "    core = s[3:]\n",
    "    # Case: digits (with optional trailing letters), e.g. \"02\", \"07N\", \"17S\"\n",
    "    m = re.match(r'^(\\d+)([A-Z].*)?$', core)\n",
    "    if m:\n",
    "        num = str(int(m.group(1)))  # drops leading zeros\n",
    "        tail = m.group(2) or ''\n",
    "        return \"WBA\" + num + tail\n",
    "    # Case: leading zeros before letters (rare), e.g. \"0N\" -> \"N\"\n",
    "    m2 = re.match(r'^0+([A-Z].*)$', core)\n",
    "    if m2:\n",
    "        return \"WBA\" + m2.group(1)\n",
    "    # Fallback: leave core as-is\n",
    "    return \"WBA\" + core\n",
    "\n",
    "# =======================\n",
    "# 1) Load inputs\n",
    "# =======================\n",
    "gw1_df = load_gw1_df(INPUT_GW_CSV)\n",
    "\n",
    "wba_df = pd.read_csv(WBA_FILE)\n",
    "required_cols = {\"fid\", \"GIS_Acres\", \"WBA_ID\"}\n",
    "missing = required_cols - set(wba_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CalSim3_WBA.csv must contain {required_cols}, missing {missing}\")\n",
    "\n",
    "# Maps from WBA table\n",
    "sr_to_fid    = {f\"SR{int(fid):02d}\": fid for fid in wba_df[\"fid\"]}\n",
    "fid_to_acres = dict(zip(wba_df[\"fid\"], wba_df[\"GIS_Acres\"]))\n",
    "\n",
    "# mapping_df should already exist: [\"SR_number\", \"WBA_name\"]\n",
    "# NOTE: leaving as-is per your original design — this must be defined upstream.\n",
    "sr_to_wba = dict(zip(mapping_df[\"SR_number\"], mapping_df[\"WBA_name\"]))\n",
    "\n",
    "# =======================\n",
    "# 2) Build monthly FT from gw1_df (WBAxx_s00xx, DETAW_s00xx)\n",
    "# =======================\n",
    "series_map = {}\n",
    "\n",
    "for col in gw1_df.columns:\n",
    "    if not isinstance(col, tuple) or len(col) < 5:\n",
    "        continue\n",
    "\n",
    "    model, var_tag, var_type, timestep, unit = col[:5]\n",
    "\n",
    "    # Handle WBA/SR style (values assumed TAF; convert to FT via area & 1000 AF/TAF)\n",
    "    if isinstance(var_tag, str) and re.match(r'^SR\\d+:TOT_s\\d{4}$', var_tag):\n",
    "        sr, rest = var_tag.split(\":\")\n",
    "        scen_raw = rest.split(\"_s\")[-1]\n",
    "        scenario = f\"s{int(scen_raw):04d}\"\n",
    "\n",
    "        if sr not in sr_to_wba:\n",
    "            continue\n",
    "        wba_name = sr_to_wba[sr]\n",
    "        fid = sr_to_fid.get(sr)\n",
    "        area_acres = fid_to_acres.get(fid)\n",
    "        if area_acres is None or float(area_acres) == 0.0:\n",
    "            continue\n",
    "\n",
    "        series = pd.to_numeric(gw1_df[col], errors=\"coerce\")\n",
    "        series = series[series.index >= pd.Timestamp(f\"{START_YEAR_PRECLIP}-01-01\")]\n",
    "        values_ft = (series / float(area_acres)) * 1000.0  # TAF -> AF -> FT\n",
    "\n",
    "        neg_idx = np.where(values_ft < 0)[0]\n",
    "        if len(neg_idx) > 0:\n",
    "            values_ft = values_ft.iloc[:neg_idx[0]]\n",
    "\n",
    "        simple_name = f\"{wba_name}_{scenario}\"\n",
    "        series_map[simple_name] = values_ft\n",
    "\n",
    "    # Handle DETAW style (convert AF → FT by dividing by DETAW area)\n",
    "    elif isinstance(var_tag, str) and re.match(r'^DETAW:TOT_s\\d{4}$', var_tag):\n",
    "        sr, rest = var_tag.split(\":\")\n",
    "        scen_raw = rest.split(\"_s\")[-1]\n",
    "        scenario = f\"s{int(scen_raw):04d}\"\n",
    "\n",
    "        series_af = pd.to_numeric(gw1_df[col], errors=\"coerce\")\n",
    "        series_af = series_af[series_af.index >= pd.Timestamp(f\"{START_YEAR_PRECLIP}-01-01\")]\n",
    "\n",
    "        # Determine area to normalize DETAW AF to FT\n",
    "        if DETAW_AREA_ACRES is not None:\n",
    "            detaw_area = float(DETAW_AREA_ACRES)\n",
    "        elif USE_TOTAL_WBA_ACRES_FOR_DETAW:\n",
    "            detaw_area = float(pd.to_numeric(wba_df[\"GIS_Acres\"], errors=\"coerce\").sum())\n",
    "        else:\n",
    "            raise ValueError(\"DETAW area is not set. Provide DETAW_AREA_ACRES or enable USE_TOTAL_WBA_ACRES_FOR_DETAW.\")\n",
    "\n",
    "        if detaw_area <= 0:\n",
    "            raise ValueError(f\"Invalid DETAW area: {detaw_area}\")\n",
    "\n",
    "        values_ft = series_af / detaw_area  # AF / acre = ft\n",
    "\n",
    "        simple_name = f\"DETAW_{scenario}\"\n",
    "        series_map[simple_name] = values_ft\n",
    "\n",
    "if series_map:\n",
    "    monthly_from_tot = pd.concat(series_map, axis=1)\n",
    "    monthly_from_tot = monthly_from_tot.loc[WINDOW_START:WINDOW_END]\n",
    "else:\n",
    "    monthly_from_tot = pd.DataFrame(index=getattr(gw1_df, \"index\", pd.DatetimeIndex([])))\n",
    "\n",
    "# =======================\n",
    "# 3) Build s0000 from storage\n",
    "# =======================\n",
    "storage_df = pd.read_csv(\n",
    "    STORAGE_FILE,\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    "    low_memory=False,\n",
    "    na_values=[\"\", \"NA\", \"NaN\", \"nan\", \"-\", \"--\"]\n",
    ")\n",
    "\n",
    "wba_df[\"WBA_ID_norm\"] = wba_df[\"WBA_ID\"].apply(normalize_id)\n",
    "acres_map = (\n",
    "    wba_df[[\"WBA_ID_norm\", \"GIS_Acres\"]]\n",
    "    .dropna(subset=[\"GIS_Acres\"])\n",
    "    .drop_duplicates(subset=[\"WBA_ID_norm\"])\n",
    "    .set_index(\"WBA_ID_norm\")[\"GIS_Acres\"]\n",
    "    .astype(\"float64\")\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "s0000_result = {}\n",
    "for raw_col in storage_df.columns:\n",
    "    norm = normalize_storage_col(raw_col)\n",
    "    if norm is None:\n",
    "        continue\n",
    "\n",
    "    af_series = pd.to_numeric(storage_df[raw_col], errors=\"coerce\")\n",
    "\n",
    "    if norm == \"DETAW\":\n",
    "        # Convert DETAW AF → FT using the same area rule as above\n",
    "        if DETAW_AREA_ACRES is not None:\n",
    "            detaw_area = float(DETAW_AREA_ACRES)\n",
    "        elif USE_TOTAL_WBA_ACRES_FOR_DETAW:\n",
    "            detaw_area = float(pd.to_numeric(wba_df[\"GIS_Acres\"], errors=\"coerce\").sum())\n",
    "        else:\n",
    "            raise ValueError(\"DETAW area is not set for s0000 conversion.\")\n",
    "\n",
    "        if detaw_area <= 0:\n",
    "            raise ValueError(f\"Invalid DETAW area for s0000: {detaw_area}\")\n",
    "\n",
    "        ft_series = af_series / detaw_area  # AF / acre = ft\n",
    "        s0000_result[\"DETAW_s0000\"] = ft_series\n",
    "    else:\n",
    "        if norm not in acres_map:\n",
    "            continue\n",
    "        acres = float(acres_map[norm])\n",
    "        if acres <= 0.0:\n",
    "            continue\n",
    "        ft_series = af_series / acres  # AF / acre = ft\n",
    "        # s0000_result[f\"WBA{norm}_s0000\"] = ft_series\n",
    "\n",
    "        wba_name_clean = normalize_wba_name(f\"WBA{norm}\")\n",
    "        s0000_result[f\"{wba_name_clean}_s0000\"] = ft_series\n",
    "\n",
    "if s0000_result:\n",
    "    s0000_df = pd.DataFrame(s0000_result, index=storage_df.index)\n",
    "    s0000_df = s0000_df.loc[WINDOW_START:WINDOW_END]\n",
    "else:\n",
    "    s0000_df = pd.DataFrame(index=storage_df.index)\n",
    "\n",
    "# =======================\n",
    "# 4) Combine\n",
    "# =======================\n",
    "combined_monthly = pd.concat([monthly_from_tot, s0000_df], axis=1).sort_index(axis=1)\n",
    "combined_monthly = combined_monthly.loc[WINDOW_START:WINDOW_END]\n",
    "\n",
    "\n",
    "if not combined_monthly.empty:\n",
    "    combined_annual = combined_monthly.resample(\"YE\").mean()\n",
    "    combined_annual.index = combined_annual.index.year\n",
    "else:\n",
    "    combined_annual = pd.DataFrame(index=[])\n",
    "\n",
    "\n",
    "# --- Save FT dataframes ---\n",
    "combined_monthly_path = os.path.join(data_output_dir, monthly_ft_filename)\n",
    "combined_annual_path = os.path.join(data_output_dir, annual_ft_filename)\n",
    "combined_monthly.to_csv(combined_monthly_path)\n",
    "combined_annual.to_csv(combined_annual_path)\n",
    "\n",
    "print(\"✓ Saved Feet versions:\")\n",
    "print(f\"  Monthly FT: {combined_monthly_path}\")\n",
    "print(combined_monthly.head(5))\n",
    "print(f\"  Annual  FT: {combined_annual_path}\")\n",
    "print(combined_annual.head(5))\n",
    "\n",
    "# MODIFIED (removed)\n",
    "# combined_monthly.to_csv(combined_monthly_path)\n",
    "# combined_annual.to_csv(combined_annual_path)\n",
    "\n",
    "# =======================\n",
    "# 4a) Convert to Acre-Feet (AF)\n",
    "# =======================\n",
    "af_monthly_map = {}\n",
    "af_annual_map = {}\n",
    "\n",
    "for col in combined_monthly.columns:\n",
    "    if \"_s\" not in col:\n",
    "        continue\n",
    "\n",
    "    # Split into WBA and scenario\n",
    "    wba_part, scen_part = col.split(\"_s\", 1)\n",
    "    scenario = f\"s{scen_part}\"\n",
    "\n",
    "    # --- Determine area in acres ---\n",
    "    if wba_part == \"DETAW\":\n",
    "        if DETAW_AREA_ACRES is not None:\n",
    "            area_acres = float(DETAW_AREA_ACRES)\n",
    "        elif USE_TOTAL_WBA_ACRES_FOR_DETAW:\n",
    "            area_acres = float(pd.to_numeric(wba_df[\"GIS_Acres\"], errors=\"coerce\").sum())\n",
    "        else:\n",
    "            raise ValueError(\"DETAW area not set for AF conversion.\")\n",
    "    else:\n",
    "        wba_id = wba_part.replace(\"WBA\", \"\")\n",
    "        fid = None\n",
    "        try:\n",
    "            fid = int(wba_id)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        area_acres = fid_to_acres.get(fid) or acres_map.get(wba_id)\n",
    "\n",
    "    if area_acres is None or float(area_acres) <= 0:\n",
    "        continue\n",
    "\n",
    "    # --- Convert ft → AF ---\n",
    "    ft_series = combined_monthly[col]\n",
    "    af_series = ft_series * float(area_acres)\n",
    "\n",
    "    af_monthly_map[col] = af_series\n",
    "    af_annual_map[col] = af_series.resample(\"YE\").sum(min_count=1)\n",
    "\n",
    "# Build in one concatenation (fast, avoids fragmentation)\n",
    "combined_af_monthly = pd.concat(af_monthly_map, axis=1)\n",
    "combined_af_annual = pd.concat(af_annual_map, axis=1)\n",
    "\n",
    "# MODIFIED (added)\n",
    "combined_af_annual.index = combined_af_annual.index.year\n",
    "\n",
    "# --- Save AF dataframes ---\n",
    "combined_af_monthly_path = os.path.join(data_output_dir, monthly_af_filename)\n",
    "combined_af_annual_path  = os.path.join(data_output_dir, annual_af_filename)\n",
    "combined_af_monthly.to_csv(combined_af_monthly_path)\n",
    "combined_af_annual.to_csv(combined_af_annual_path)\n",
    "\n",
    "print(\"✓ Saved Acre-Feet versions:\")\n",
    "print(f\"  Monthly AF: {combined_af_monthly_path}\")\n",
    "print(combined_af_monthly.head(5))\n",
    "print(f\"  Annual  AF: {combined_af_annual_path}\")\n",
    "print(combined_af_annual.head(5))\n",
    "\n",
    "# =======================\n",
    "# 4b) Express as Percentage of Baseline\n",
    "# =======================\n",
    "\n",
    "pct_monthly_map = {}\n",
    "pct_annual_map = {}\n",
    "\n",
    "for col in combined_af_monthly.columns:\n",
    "    if \"_s\" not in col:\n",
    "        continue\n",
    "\n",
    "    # Split into WBA and scenario\n",
    "    wba_part, scen_part = col.split(\"_s\", 1)\n",
    "    scenario_num = int(scen_part)\n",
    "    \n",
    "    # Determine baseline scenario column\n",
    "    baseline_scen_num = baseline_scenario  # assumed to be defined elsewhere\n",
    "    baseline_col = f\"{wba_part}_{baseline_scen_num}\"\n",
    "    # print(\"Baseline column: \", baseline_col)\n",
    "    \n",
    "    if baseline_col not in combined_af_monthly.columns:\n",
    "        # Skip if baseline column doesn't exist\n",
    "        print(\"Baseline column does not exist in DF!\", baseline_col)\n",
    "        continue\n",
    "\n",
    "    # --- Compute percentage relative to baseline ---\n",
    "    baseline_series = combined_af_monthly[baseline_col]\n",
    "    pct_series = (combined_af_monthly[col] / baseline_series) * 100\n",
    "\n",
    "    pct_monthly_map[col] = pct_series\n",
    "    pct_annual_map[col] = pct_series.resample(\"YE\").mean()  # annual average percent\n",
    "\n",
    "# Build in one concatenation\n",
    "combined_volpct_monthly = pd.concat(pct_monthly_map, axis=1)\n",
    "combined_volpct_annual = pd.concat(pct_annual_map, axis=1)\n",
    "\n",
    "# MODIFIED (added)\n",
    "combined_volpct_annual.index = combined_volpct_annual.index.year\n",
    "\n",
    "# --- Save PERCENT dataframes ---\n",
    "combined_volper_monthly_path = os.path.join(data_output_dir, monthly_volper_filename)\n",
    "combined_volper_annual_path  = os.path.join(data_output_dir, annual_volper_filename)\n",
    "combined_volpct_monthly.to_csv(combined_volper_monthly_path)\n",
    "combined_volpct_annual.to_csv(combined_volper_annual_path)\n",
    "\n",
    "print(\"✓ Saved VOL PER versions:\")\n",
    "print(f\"  Monthly VOL PERCENT: {combined_volper_monthly_path}\")\n",
    "print(combined_volpct_monthly.head(5))\n",
    "print(f\"  Annual  VOL PERCENT: {combined_volper_annual_path}\")\n",
    "print(combined_volpct_annual.head(5))\n",
    "\n",
    "# =======================\n",
    "# 4c) Compute CV for AF dataframes\n",
    "# =======================\n",
    "cv_af_monthly_df = compute_cv_df(combined_af_monthly)\n",
    "cv_af_annual_df  = compute_cv_df(combined_af_annual)\n",
    "\n",
    "# --- Save CV dataframes ---\n",
    "cv_af_monthly_path = os.path.join(data_output_dir, monthly_af_cv_filename)\n",
    "cv_af_annual_path  = os.path.join(data_output_dir, annual_af_cv_filename)\n",
    "cv_af_monthly_df.to_csv(cv_af_monthly_path)\n",
    "cv_af_annual_df.to_csv(cv_af_annual_path)\n",
    "\n",
    "print(\"✓ Saved Acre-Feet CV dataframes:\")\n",
    "print(f\"  Monthly AF CV: {cv_af_monthly_path}\")\n",
    "print(cv_af_monthly_df.head(5))\n",
    "print(f\"  Annual  AF CV: {cv_af_annual_path}\")\n",
    "print(cv_af_annual_df.head(5))\n",
    "\n",
    "# =======================\n",
    "# 4d) Compute CV for FT dataframes\n",
    "# =======================\n",
    "cv_monthly_df = compute_cv_df(combined_monthly)\n",
    "cv_annual_df = compute_cv_df(combined_annual)\n",
    "\n",
    "# --- Save to CSV ---\n",
    "cv_monthly_path = os.path.join(data_output_dir, monthly_ft_cv_filename)\n",
    "cv_annual_path = os.path.join(data_output_dir, annual_ft_cv_filename)\n",
    "\n",
    "cv_monthly_df.to_csv(cv_monthly_path)\n",
    "cv_annual_df.to_csv(cv_annual_path)\n",
    "\n",
    "print(\"✓ Saved FT CV dataframes:\")\n",
    "print(\"Monthly FT CV:\", cv_monthly_path)\n",
    "print(\"Annual FT CV:\", cv_annual_path)\n",
    "display(cv_annual_df.head())\n",
    "\n",
    "# =======================\n",
    "# 4e) Compute CV for PER VOL dataframes\n",
    "# =======================\n",
    "volpercv_monthly_df = compute_cv_df(combined_volpct_monthly)\n",
    "volpercv_annual_df  = compute_cv_df(combined_volpct_monthly)\n",
    "\n",
    "# --- Save CV dataframes ---\n",
    "volpercv_monthly_path = os.path.join(data_output_dir, monthly_volpercv_filename)\n",
    "volpercv_annual_path  = os.path.join(data_output_dir, annual_volpercv_filename)\n",
    "volpercv_monthly_df.to_csv(volpercv_monthly_path)\n",
    "volpercv_annual_df.to_csv(volpercv_annual_path)\n",
    "\n",
    "print(\"✓ Saved VOL PER CV dataframes:\")\n",
    "print(f\"  Monthly VOL PER CV: {volpercv_monthly_path}\")\n",
    "display(volpercv_monthly_df.head())\n",
    "print(f\"  Annual  VOL PER CV: {volpercv_annual_path}\")\n",
    "display(volpercv_annual_df.head())\n",
    "\n",
    "# MODIFIED (the following lines are alread commented, but can be removed entirely)# # =======================\n",
    "# # 4f) Compute CV for PER LEV dataframes\n",
    "# # =======================\n",
    "# levpercv_monthly_df = compute_cv_df(combined_levpct_monthly)\n",
    "# levpercv_annual_df  = compute_cv_df(combined_levpct_monthly)\n",
    "\n",
    "# # --- Save CV dataframes ---\n",
    "# levpercv_monthly_path = os.path.join(data_output_dir, monthly_levpercv_filename)\n",
    "# levpercv_annual_path  = os.path.join(data_output_dir, annual_levpercv_filename)\n",
    "# levpercv_monthly_df.to_csv(levpercv_monthly_path)\n",
    "# levpercv_annual_df.to_csv(levpercv_annual_path)\n",
    "\n",
    "# print(\"✓ Saved LEV PER CV dataframes:\")\n",
    "# print(f\"  Monthly LEV PER CV: {levpercv_monthly_path}\")\n",
    "# print(f\"  Annual  LEV PER CV: {levpercv_annual_path}\")\n",
    "# display(volpercv_annual_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b7ed0-7400-47c1-a53d-00f8c3763432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "# --- Time variable in months since start (assumes combined_monthly is monthly) ---\n",
    "time_numeric = np.arange(len(combined_monthly)).reshape(-1, 1)\n",
    "\n",
    "# --- Compute slopes (ft/month) and and last 10 years avg - first 10 avg values (ft) for every WBA_s#### column ---\n",
    "records = []\n",
    "for col in combined_monthly.columns:\n",
    "    if \"_s\" not in col:\n",
    "        continue\n",
    "    wba_raw, scen_raw = col.split(\"_s\", 1)  # e.g., \"WBA02\", \"0001\"\n",
    "    scenario = f\"s{scen_raw}\"\n",
    "    ts = combined_monthly[col].dropna()\n",
    "    y = combined_monthly[col].to_numpy(dtype=float)\n",
    "    mask = ~np.isnan(y)\n",
    "    \n",
    "    if mask.sum() > 1:\n",
    "        # compute trend\n",
    "        model = LinearRegression().fit(time_numeric[mask], y[mask])\n",
    "        slope = model.coef_[0]  # ft/month\n",
    "        wba_norm = normalize_wba_name(wba_raw)\n",
    "        records.append({\"scenario\": scenario, \"WBA\": wba_norm, \"slope_ft_per_month\": slope})\n",
    "\n",
    "        # compute n-year start vs end means\n",
    "        ts = ts.sort_index()  # ensure chronological order\n",
    "        years = ts.index.year\n",
    "        first_years = ts[years <= years.min() + YearsToAverage - 1]\n",
    "        last_years = ts[years >= years.max() - YearsToAverage + 1]\n",
    "    \n",
    "        if len(first_years) > 0 and len(last_years) > 0:\n",
    "            mean_start = first_years.mean()\n",
    "            mean_end = last_years.mean()\n",
    "            diff_ft = mean_end - mean_start  # ft\n",
    "        else:\n",
    "            diff_ft = np.nan\n",
    "\n",
    "        # --- Store results ---\n",
    "        records.append({\n",
    "            \"scenario\": scenario,\n",
    "            \"WBA\": wba_norm,\n",
    "            \"slope_ft_per_month\": slope,\n",
    "            \"mean_start_ft\": mean_start,\n",
    "            \"mean_end_ft\": mean_end,\n",
    "            \"diff_last_vs_first_ft\": diff_ft\n",
    "        })\n",
    "        \n",
    "records_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# --- If duplicates appear after normalization (e.g., WBA07N & WBA7N), merge by mean ---\n",
    "trends_df = (\n",
    "    records_df\n",
    "    .groupby([\"scenario\", \"WBA\"], as_index=False)[\"slope_ft_per_month\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "diff_df = (\n",
    "    records_df\n",
    "    .groupby([\"scenario\", \"WBA\"], as_index=False)[\"diff_last_vs_first_ft\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# --- Pivot: scenarios as rows, WBAs as columns ---\n",
    "trend_matrix = trends_df.pivot(index=\"scenario\", columns=\"WBA\", values=\"slope_ft_per_month\")\n",
    "diff_matrix = diff_df.pivot(index=\"scenario\", columns=\"WBA\", values=\"diff_last_vs_first_ft\")\n",
    "\n",
    "# --- (Optional) tidy sorting: scenarios numerically, WBA columns by numeric then suffix ---\n",
    "def scen_key(s):  # \"s0001\" -> 1\n",
    "    try:\n",
    "        return int(str(s).lstrip(\"sS\"))\n",
    "    except Exception:\n",
    "        return 10**9\n",
    "\n",
    "def wba_key(w):  # \"WBA17S\" -> (17, \"S\"), \"DETAW\" -> (inf, \"DETAW\")\n",
    "    w = str(w).upper()\n",
    "    if w.startswith(\"WBA\"):\n",
    "        core = w[3:]\n",
    "        m = re.match(r'^(\\d+)([A-Z].*)?$', core)\n",
    "        if m:\n",
    "            return (int(m.group(1)), m.group(2) or \"\")\n",
    "    return (10**9, w)\n",
    "\n",
    "trend_matrix = trend_matrix.reindex(sorted(trend_matrix.index, key=scen_key))\n",
    "trend_matrix = trend_matrix.reindex(sorted(trend_matrix.columns, key=wba_key), axis=1)\n",
    "diff_matrix = diff_matrix.reindex(sorted(diff_matrix.index, key=scen_key))\n",
    "diff_matrix = diff_matrix.reindex(sorted(diff_matrix.columns, key=wba_key), axis=1)\n",
    "\n",
    "# --- Save CSVs ---\n",
    "trends_out_path = os.path.join(data_output_dir, trend_filename)\n",
    "trend_matrix.to_csv(trends_out_path)\n",
    "diffs_out_path = os.path.join(data_output_dir, diff_filename)\n",
    "diff_matrix.to_csv(diffs_out_path)\n",
    "\n",
    "print(\"✓ Trends file written:\", trends_out_path)\n",
    "print(\"Shape:\", trend_matrix.shape)\n",
    "print(trend_matrix.head())\n",
    "print(\"✓ Diffs file written:\", diffs_out_path)\n",
    "print(\"Shape:\", diff_matrix.shape)\n",
    "print(diff_matrix.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f541c4-06f5-4963-8bad-2b86d63954b8",
   "metadata": {},
   "source": [
    "# Plot histograms to find a natural break in the trends of baseline scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4763b-4df4-496b-aa44-cd0fac9015af",
   "metadata": {},
   "source": [
    "## Specify number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f9ba8-bc17-4e7b-adcf-b5e9d2e7a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "nBins = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2db3f-3e27-486a-b9bc-6c8eb5f0df22",
   "metadata": {},
   "source": [
    "## Plot baseline trend histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08128a46-6551-4262-b209-fc1411e0f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "if SaveHistogram:\n",
    "    # --- Select the row from trend_matrix ---\n",
    "    baseline_data = trend_matrix.loc[baseline_scenario].dropna()\n",
    "    \n",
    "    # --- Plot histogram ---\n",
    "    bins = 20  # you can set nBins here\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts, bin_edges, _ = plt.hist(baseline_data, bins=bins, edgecolor='black')\n",
    "    \n",
    "    save_name = f\"TrendsHistogram_{baseline_scenario}.png\"\n",
    "    save_path = os.path.join(trends_output_dir, save_name)\n",
    "    \n",
    "    # Add more x-axis ticks using bin edges\n",
    "    plt.xticks(np.round(bin_edges, 4), rotation=45)\n",
    "    plt.title(f\"Histogram of Monthly Trends (ft/month) for {baseline_scenario}\")\n",
    "    plt.xlabel(\"Slope Value (ft/month)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588b3c2-49c8-4643-87e9-56cbc1079e7c",
   "metadata": {},
   "source": [
    "## Specify clipping quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003dc609-5e4b-4050-8f3d-a0181d0cb31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lQuant = 0\n",
    "hQuant = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df9df3-8e14-4d1b-a6b1-0b9c3234c06e",
   "metadata": {},
   "source": [
    "## Plot clipped baseline trend histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f8159-5bb9-478c-9083-1af752699b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "if SaveHistogram:\n",
    "\n",
    "    # --- Select the row from trend_matrix ---\n",
    "    baseline_data = trend_matrix.loc[baseline_scenario].dropna()\n",
    "    \n",
    "    # --- Quantile thresholds (set these before running) ---\n",
    "    # Example: lQuant = 0.05; hQuant = 0.95\n",
    "    lVal, hVal = np.quantile(baseline_data.values, [lQuant, hQuant])\n",
    "    \n",
    "    # --- Clip data ---\n",
    "    clipped_data = baseline_data.values.clip(lVal, hVal)\n",
    "    \n",
    "    # --- Plot histogram ---\n",
    "    bins = 20  # or nBins if you’ve defined it elsewhere\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    counts, bin_edges, _ = plt.hist(clipped_data, bins=bins, edgecolor='black')\n",
    "    \n",
    "    save_name = f\"ClippedTrendsHistogram_{baseline_scenario}.png\"\n",
    "    save_path = os.path.join(trends_output_dir, save_name)\n",
    "    \n",
    "    # Add more x-axis ticks using bin edges\n",
    "    plt.xticks(np.round(bin_edges, 4), rotation=45)\n",
    "    plt.title(f\"Histogram of Monthly Trends (ft/month) for {baseline_scenario} (after clipping)\")\n",
    "    plt.xlabel(\"Slope Value (ft/month)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b0483-4dfa-499b-a142-f7ca639c63d9",
   "metadata": {},
   "source": [
    "## Notes: Where to put the break? What threshold to use to distinguish moderate from severe? Propose: -0.015; For the GW model (s0000) we tried -0.0025; For s0002, trying -0.0075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f41d2-0867-4f3f-98b8-c67977e10fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_decline_threshold=-0.0075"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2b102-7a3a-40bb-add0-d286054596ca",
   "metadata": {},
   "source": [
    "## Plot trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab60b9-f1d8-460d-b944-71440b0c5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "if SaveTrendPlots:\n",
    "    # === Parameters ===\n",
    "    scenarios_to_plot = sorted({baseline_scenario, *index_names})\n",
    "    # scenarios_to_plot = {\"s0002\", \"s0015\"}\n",
    "    print(\"Scenarios to plot:\")\n",
    "    print(scenarios_to_plot)\n",
    "    drop_threshold = 1000\n",
    "    start_year = 1960\n",
    "    \n",
    "    # === Use combined_monthly directly ===\n",
    "    # combined_monthly should already be loaded in your workspace\n",
    "    gw1_df_filtered = combined_monthly.copy()\n",
    "    \n",
    "    # === Iterate over columns (WBA_s####) ===\n",
    "    for col in gw1_df_filtered.columns:\n",
    "        if \"_s\" not in col:\n",
    "            continue\n",
    "    \n",
    "        wba_id, scenario = col.split(\"_\")\n",
    "        if scenario not in scenarios_to_plot:\n",
    "            continue\n",
    "    \n",
    "        ts = gw1_df_filtered[col].dropna()\n",
    "        ts = ts[ts.index >= pd.Timestamp(f\"{start_year}-01-01\")]\n",
    "    \n",
    "        diffs = ts.diff()\n",
    "        drop_indices = diffs[diffs < -drop_threshold].index\n",
    "        if not drop_indices.empty:\n",
    "            cutoff_idx = drop_indices[0]\n",
    "            ts = ts[ts.index <= cutoff_idx]\n",
    "            drop_year = cutoff_idx.year\n",
    "        else:\n",
    "            drop_year = 2015  # clipped already\n",
    "    \n",
    "        if len(ts) < 2:\n",
    "            continue  # skip if not enough data\n",
    "    \n",
    "        # Fit trendline\n",
    "        x = (ts.index - ts.index[0]).days / 365.25\n",
    "        y = ts.values\n",
    "        slope, intercept = np.polyfit(x, y, 1)\n",
    "        trend = slope * x + intercept\n",
    "    \n",
    "        # === Plot ===\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(ts.index, y, label=\"Observed\", marker=\"o\")\n",
    "        plt.plot(ts.index, trend, linestyle=\"--\", label=f\"Trend (slope={slope:.6f})\")\n",
    "        \n",
    "        # Ensure title not clipped by adding pad\n",
    "        plt.title(f\"{wba_id} under {scenario} (end year: {drop_year})\", pad=20)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Groundwater Storage (FT)\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        save_name = f\"{scenario}_{wba_id}.png\"\n",
    "        save_path = os.path.join(trends_output_dir, save_name)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        print(f\"✓ Saved: {save_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957c827-266f-4d6a-8450-849b2722abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849130d-0559-4275-bbc9-ae146c53193f",
   "metadata": {},
   "source": [
    "## Trend Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977063c6-bd72-4d6c-9571-a9f8cc3be05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "\n",
    "# === Time axis (months from start to end) ===\n",
    "time_index = pd.date_range(start=\"1960-01-01\", end=\"2015-12-31\", freq=\"M\")\n",
    "t_months = np.arange(len(time_index))  # 0,1,2,...\n",
    "\n",
    "# === Time axis (clipped to match combined_monthly) ===\n",
    "time_index = pd.date_range(start=\"1973-10-31\", end=\"2015-09-30\", freq=\"M\")\n",
    "t_months = np.arange(len(time_index))  # 0,1,2,... months\n",
    "\n",
    "\n",
    "# === Function ===\n",
    "def plot_trendline_comparison_from_matrix(scenario_code, baseline_code=baseline_scenario, save_dir=None):\n",
    "    if scenario_code not in trend_matrix.index or baseline_code not in trend_matrix.index:\n",
    "        print(f\"Scenario {scenario_code} or baseline {baseline_code} not in trend_matrix\")\n",
    "        return\n",
    "\n",
    "    scenario_slopes = trend_matrix.loc[scenario_code]\n",
    "    baseline_slopes = trend_matrix.loc[baseline_code]\n",
    "\n",
    "    shared_wbas = sorted(set(scenario_slopes.dropna().index) & set(baseline_slopes.dropna().index))\n",
    "    if not shared_wbas:\n",
    "        print(f\"No shared WBAs for {scenario_code} and {baseline_code}\")\n",
    "        return\n",
    "\n",
    "    ncols = 3\n",
    "    nrows = math.ceil(len(shared_wbas) / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 3 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, wba in enumerate(shared_wbas):\n",
    "        slope = scenario_slopes[wba]\n",
    "        slope_base = baseline_slopes[wba]\n",
    "\n",
    "        # Build linear trends: start at 0, slope in ft/month\n",
    "        trend = slope * t_months\n",
    "        trend_base = slope_base * t_months\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.plot(time_index, trend, color=\"red\", linestyle=\"--\", label=f\"{scenario_code} trend\")\n",
    "        ax.plot(time_index, trend_base, color=\"black\", linestyle=\"--\", label=f\"{baseline_code} trend\")\n",
    "\n",
    "        ax.set_title(wba)\n",
    "        ax.set_xlim(time_index[0], time_index[-1])\n",
    "        ax.set_ylabel(\"FT (relative)\")\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Annotate slope values\n",
    "        ax.text(0.01, 0.95, f\"{scenario_code} slope: {slope:.6f}\", transform=ax.transAxes,\n",
    "                fontsize=8, color=\"red\", verticalalignment='top')\n",
    "        ax.text(0.01, 0.85, f\"{baseline_code} slope: {slope_base:.6f}\", transform=ax.transAxes,\n",
    "                fontsize=8, color=\"black\", verticalalignment='top')\n",
    "\n",
    "        ax.legend(loc=\"lower right\", fontsize=7, frameon=True)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"Trendline Comparison (ft/month): {scenario_code} vs {baseline_code}\", fontsize=15)\n",
    "    fig.tight_layout(rect=[0, 0.04, 1, 0.96])\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"{scenario_code}_vs_{baseline_code}_trendlines.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"✓ Saved: {save_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# === Call for all scenarios except baseline ===\n",
    "if SaveTrendComparisonsPlots:\n",
    "    all_scenarios = [sc for sc in trend_matrix.index if sc != baseline_scenario]\n",
    "    for sc in all_scenarios:\n",
    "        plot_trendline_comparison_from_matrix(scenario_code=sc, baseline_code=baseline_scenario, save_dir=trends_comparisons_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575de9b7-295d-42e9-905a-c9e75b3adc53",
   "metadata": {},
   "source": [
    "## Compute tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f283324-38b1-4104-be2e-796925eb5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "def assign_tiers_from_trends(trend_matrix, baseline, output_dir, filename, severe_decline_threshold=-0.015):\n",
    "    if baseline not in trend_matrix.index:\n",
    "        raise ValueError(f\"Baseline scenario {baseline} not found in trend_matrix\")\n",
    "\n",
    "    tier_matrix = pd.DataFrame(index=trend_matrix.index, columns=trend_matrix.columns)\n",
    "\n",
    "    for wba_col in trend_matrix.columns:\n",
    "        baseline_slope = trend_matrix.loc[baseline, wba_col]\n",
    "\n",
    "        for scenario in trend_matrix.index:\n",
    "            slope = trend_matrix.loc[scenario, wba_col]\n",
    "\n",
    "            if pd.isna(slope) or pd.isna(baseline_slope):\n",
    "                tier = np.nan\n",
    "            elif scenario == baseline:\n",
    "                tier = 0  # baseline tier\n",
    "            elif slope >= 0:\n",
    "                diff = slope - baseline_slope\n",
    "                tier = 1 if diff >= 0 else 2\n",
    "            elif slope >= severe_decline_threshold:\n",
    "                tier = 3\n",
    "            else:\n",
    "                tier = 4\n",
    "\n",
    "            tier_matrix.loc[scenario, wba_col] = tier\n",
    "\n",
    "    # === Save results ===\n",
    "    out_path = os.path.join(output_dir, filename)\n",
    "    tier_matrix.to_csv(out_path)\n",
    "    print(\"✓ Tier assignment saved to:\", out_path)\n",
    "    print(tier_matrix.head())\n",
    "\n",
    "    return tier_matrix\n",
    "\n",
    "# Call the function\n",
    "tier_matrix = assign_tiers_from_trends(\n",
    "    trend_matrix,\n",
    "    baseline=baseline_scenario,\n",
    "    output_dir=data_output_dir,\n",
    "    filename=tier_filename,\n",
    "    severe_decline_threshold=severe_decline_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395da89-68ab-439e-9ec5-d474ba5cd678",
   "metadata": {},
   "source": [
    "## Tier maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644c308-88ea-4009-ac61-ccc493b84be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "if SaveTierMaps:\n",
    "    wba_shp = gpd.read_file(shapefile_path)\n",
    "    wba_shp[\"WBA_ID\"] = wba_shp[\"WBA_ID\"].str.strip()\n",
    "    \n",
    "    # === Load new tier assignment CSV ===\n",
    "    tier_df = pd.read_csv(tier_output_path, index_col=0)\n",
    "    \n",
    "    # === Fix WBA column names to match shapefile format ===\n",
    "    new_columns = {}\n",
    "    for col in tier_df.columns:\n",
    "        if col.startswith(\"WBA\"):\n",
    "            suffix = col[3:]\n",
    "            if suffix.isdigit():\n",
    "                new_columns[col] = suffix.zfill(2)\n",
    "            else:\n",
    "                digits = ''.join(filter(str.isdigit, suffix)).zfill(2)\n",
    "                letter = ''.join(filter(str.isalpha, suffix))\n",
    "                new_columns[col] = digits + letter\n",
    "    tier_df.rename(columns=new_columns, inplace=True)\n",
    "    \n",
    "    # === Define atlas-style muted tier colors ===\n",
    "    tier_colors = {\n",
    "        1: \"#B5CDA3\",  # olive green\n",
    "        2: \"#8FBBD9\",  # soft dusty blue\n",
    "        3: \"#E6C27A\",  # warm khaki\n",
    "        4: \"#D97B6D\",  # soft brick red (worst)\n",
    "    }\n",
    "    \n",
    "    # === Vertical shifts for selected WBA labels ===\n",
    "    label_shifts = {\n",
    "        \"17STOT\": 0.015,\n",
    "        \"71TOT\": 0.015,\n",
    "        \"22TOT\": 0.015,\n",
    "        \"50TOT\": -0.015,\n",
    "        \"21TOT\": -0.015,\n",
    "        \"12TOT\": -0.015,\n",
    "    }\n",
    "    \n",
    "    # === Plot and save maps for each scenario (skip baseline s0000) ===\n",
    "    for scenario in tier_df.index:\n",
    "        if scenario == baseline_scenario:  # skip baseline\n",
    "            continue\n",
    "    \n",
    "        # Map tier data to shapefile\n",
    "        tier_series = tier_df.loc[scenario]\n",
    "        tier_map = wba_shp.copy()\n",
    "        tier_map[\"GroundwaterTier\"] = tier_map[\"WBA_ID\"].map(tier_series.to_dict())\n",
    "    \n",
    "        # Plot base\n",
    "        fig, ax = plt.subplots(figsize=(8, 10))\n",
    "        for tier_val, color in tier_colors.items():\n",
    "            subset = tier_map[tier_map[\"GroundwaterTier\"] == tier_val]\n",
    "            if not subset.empty:\n",
    "                subset.plot(\n",
    "                    ax=ax,\n",
    "                    color=color,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=0.3,\n",
    "                    label=f\"Tier {tier_val}\"\n",
    "                )\n",
    "    \n",
    "        # === Add WBA_ID labels ===\n",
    "        for idx, row in tier_map.iterrows():\n",
    "            if pd.notna(row[\"GroundwaterTier\"]):\n",
    "                x, y = row.geometry.centroid.x, row.geometry.centroid.y\n",
    "                wba_id = row[\"WBA_ID\"]\n",
    "                shift = label_shifts.get(wba_id, 0)\n",
    "                ax.text(\n",
    "                    x, y + shift, wba_id,\n",
    "                    fontsize=7, weight='bold', ha='center'\n",
    "                )\n",
    "    \n",
    "        ax.set_title(f\"Groundwater Tiers for Scenario {scenario}\", fontweight=\"bold\")\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "        # Add legend\n",
    "        legend_handles = [mpatches.Patch(color=color, label=f\"Tier {tier}\")\n",
    "                          for tier, color in tier_colors.items()]\n",
    "        ax.legend(handles=legend_handles, title=\"Tier\", loc=\"lower left\", frameon=True)\n",
    "    \n",
    "        # Save and show\n",
    "        save_path = os.path.join(tiers_output_dir, f\"GroundWaterTiers_{scenario}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        print(f\"✓ Saved map for {scenario} to: {output_dir}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6889eb20-57f9-4cce-8afa-71bb9318a433",
   "metadata": {},
   "source": [
    "## Slope rank map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a49d7a-bd30-4a03-b26b-91d38d191f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFIED\n",
    "# --- Helper to pad WBA IDs ---\n",
    "def pad_index(idx):\n",
    "    import re\n",
    "    if isinstance(idx, str):\n",
    "        m = re.match(r'^(\\d{1,2})([A-Z]*)$', idx)\n",
    "        if m:\n",
    "            num = int(m.group(1))\n",
    "            suffix = m.group(2)\n",
    "            return f\"{num:02d}{suffix}\"\n",
    "    return idx\n",
    "\n",
    "if SaveSlopeMap:\n",
    "    # === Load shapefile ===\n",
    "    wba_shp = gpd.read_file(shapefile_path)\n",
    "    wba_shp[\"WBA_ID\"] = wba_shp[\"WBA_ID\"].str.strip().str.upper()\n",
    "    \n",
    "    # === Load new slopes CSV (trend_matrix output) ===\n",
    "    trend_csv = trends_out_path\n",
    "    \n",
    "    df = pd.read_csv(trend_csv, index_col=0)\n",
    "    \n",
    "    # === Scenarios to plot ===\n",
    "    scenarios_to_plot = [baseline_scenario]\n",
    "    \n",
    "    for scenario in scenarios_to_plot:\n",
    "        if scenario not in df.index:\n",
    "            print(f\"⚠ Scenario {scenario} not found in {trend_csv}\")\n",
    "            continue\n",
    "    \n",
    "        baseline_data = df.loc[scenario].dropna()\n",
    "    \n",
    "        # --- Clean WBA names ---\n",
    "        baseline_data.index = (\n",
    "            baseline_data.index\n",
    "            .str.replace(\"WBA\", \"\", regex=False)\n",
    "            .str.replace(\":TOT\", \"\", regex=False)\n",
    "            .str.replace(\"TOT\", \"\", regex=False)\n",
    "            .str.lstrip(\"0\")\n",
    "            .str.strip()\n",
    "            .str.upper()\n",
    "        )\n",
    "        baseline_data.index = baseline_data.index.map(pad_index)\n",
    "    \n",
    "        # === Join slope data to shapefile ===\n",
    "        slope_rank = baseline_data.rank().astype(int)\n",
    "        slope_map = wba_shp.copy()\n",
    "        slope_map[\"Slope\"] = slope_map[\"WBA_ID\"].map(baseline_data.to_dict())\n",
    "        slope_map[\"SlopeRank\"] = slope_map[\"WBA_ID\"].map(slope_rank.to_dict())\n",
    "    \n",
    "        # === Color setup ===\n",
    "        num_ranks = slope_rank.nunique()\n",
    "        cmap = plt.colormaps.get_cmap(\"coolwarm_r\").resampled(num_ranks)\n",
    "        norm = mcolors.Normalize(vmin=1, vmax=num_ranks)\n",
    "    \n",
    "        # === Plot map ===\n",
    "        fig, ax = plt.subplots(figsize=(8, 10))\n",
    "        slope_map.plot(\n",
    "            column=\"SlopeRank\",\n",
    "            cmap=cmap,\n",
    "            linewidth=0.3,\n",
    "            edgecolor='black',\n",
    "            ax=ax,\n",
    "            legend=False,\n",
    "            norm=norm\n",
    "        )\n",
    "    \n",
    "        # Add labels\n",
    "        for idx, row in slope_map.iterrows():\n",
    "            if pd.notna(row[\"SlopeRank\"]):\n",
    "                x, y = row.geometry.centroid.x, row.geometry.centroid.y\n",
    "                ax.text(x, y, row[\"WBA_ID\"], fontsize=7, weight='bold', ha='center')\n",
    "    \n",
    "        # Add colorbar with slope values\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm._A = []\n",
    "        rank_to_slope = slope_map.dropna(subset=[\"Slope\", \"SlopeRank\"]) \\\n",
    "                                 .groupby(\"SlopeRank\")[\"Slope\"].mean().sort_index()\n",
    "        tick_locs = list(rank_to_slope.index)\n",
    "        tick_labels = [f\"{s:.6f}\" for s in rank_to_slope.values]\n",
    "    \n",
    "        cbar = fig.colorbar(sm, ax=ax, orientation=\"vertical\", ticks=tick_locs)\n",
    "        cbar.set_label(\"Slope Value (ft/month)\")\n",
    "        cbar.ax.yaxis.set_major_locator(FixedLocator(tick_locs))\n",
    "        cbar.ax.yaxis.set_major_formatter(FixedFormatter(tick_labels))\n",
    "    \n",
    "        ax.set_title(f\"Slope Rank Map for Scenario {scenario}\", fontweight=\"bold\", pad=15)\n",
    "        ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "    \n",
    "        # === Save ===\n",
    "        save_path = os.path.join(tiers_output_dir, f\"SlopeRankMap_{scenario}.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        print(f\"✓ Saved slope rank map for {scenario} to {output_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa5787-6188-4eae-904c-0b00c06378e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7736398-04ce-45a1-bbda-00ef764bde3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
