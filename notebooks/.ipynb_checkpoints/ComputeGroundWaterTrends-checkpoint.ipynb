{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a9bc43-c6c3-4103-a45d-55b7a2938c13",
   "metadata": {},
   "source": [
    "## Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c91bd1a1e8beca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T08:04:38.276992Z",
     "start_time": "2024-07-20T08:04:38.274280Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# append coeqwal packages to path\n",
    "sys.path.append('./coeqwalpackage')\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cqwlutils as cu\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9145a-2421-4dd1-a31d-1edb2b5a4f64",
   "metadata": {},
   "source": [
    "## Import custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61561cb3284fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-07T20:35:57.550559300Z",
     "start_time": "2024-03-07T20:35:57.462942100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import custom modules - NEED WINDOWS OS (NOTE: I had to run this twice, must check why this happens!)\n",
    "from coeqwalpackage.DataExtraction import *\n",
    "from coeqwalpackage.metrics import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a886dad8c6c902",
   "metadata": {},
   "source": [
    "## Define contol file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0f49b-17c9-40f5-8861-5680673a5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "CtrlFile = 'CalSim3GroundWaterDataExtractionInitFile_v1.xlsx'\n",
    "CtrlTab = 'Init'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070e30e-fec8-4e22-bf9f-bd43e1d4f7a9",
   "metadata": {},
   "source": [
    "## Read from control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614d95e-3b73-42f9-a330-7750b45b921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ScenarioListFile, ScenarioListTab, ScenarioListPath, GW1DssNamesOutPath, GW2DssNamesOutPath, ScenarioIndicesOutPath, DssDirsOutPath, VarListPath, VarListFile, VarListTab, VarOutPath, DataOutPath, ConvertDataOutPath, ExtractionSubPath, DemandDeliverySubPath, ModelSubPath, GroupDataDirPath, ScenarioDir, GW1DssMin, GW1DssMax, GW2DssMin, GW2DssMax, NameMin, NameMax, DirMin, DirMax, IndexMin, IndexMax, StartMin, StartMax, EndMin, EndMax, VarMin, VarMax, DemandFilePath, DemandFileName, DemandFileTab, DemMin, DemMax, InflowOutSubPath, InflowFilePath, InflowFileName, InflowFileTab, InflowMin, InflowMax = cu.read_init_file(CtrlFile, CtrlTab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2529c38-c663-43ee-811d-415fdd815743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([ScenarioListFile, ScenarioListTab, ScenarioListPath, GW1DssNamesOutPath, GW2DssNamesOutPath, ScenarioIndicesOutPath, DssDirsOutPath, VarListPath, VarListFile, VarListTab, VarOutPath, DataOutPath, ConvertDataOutPath, ExtractionSubPath, DemandDeliverySubPath, ModelSubPath, GroupDataDirPath, ScenarioDir, GW1DssMin, GW1DssMax, GW2DssMin, GW2DssMax, NameMin, NameMax, DirMin, DirMax, IndexMin, IndexMax, StartMin, StartMax, EndMin, EndMax, VarMin, VarMax, DemandFilePath, DemandFileName, DemandFileTab, DemMin, DemMax, InflowOutSubPath, InflowFilePath, InflowFileName, InflowFileTab, InflowMin, InflowMax])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd54452-9c03-4f71-ba08-06fe3b8ba071",
   "metadata": {},
   "source": [
    "## Check for output directory and create if necessary (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bb493-d379-4c25-a56e-e3da4fab4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if output directory exists\n",
    "if not os.path.exists(GroupDataDirPath):\n",
    "    # print warning\n",
    "    print(\"Warning: directory \" + GroupDataDirPath + \" does not exists and will be created\")\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(GroupDataDirPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429f736a-622a-4ca0-86f7-8979963700bf",
   "metadata": {},
   "source": [
    "## Define Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17492ae9-5a01-4eb2-b6e1-4422fbd39d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN values as defined by CalSim3\n",
    "Nan1 = -901\n",
    "Nan2 = -902"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02cf686-69bd-4a22-bc35-42769740aae7",
   "metadata": {},
   "source": [
    "## Read indeces, dss names, directory names, start and end dates, time range (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2a998-acab-49e7-a2bd-25e6336f0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw1dsshdr, gw1dssname = cu.read_from_excel(ScenarioListPath, ScenarioListTab, GW1DssMin, GW1DssMax, hdr=True)\n",
    "gw1dss_names = []\n",
    "for i in range(len(gw1dssname)):\n",
    "    gw1dss_names.append(gw1dssname[i][0])\n",
    "gw1dss_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9511228c-0d07-4308-86e7-bc1f9a3f74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw2dsshdr, gw2dssname = cu.read_from_excel(ScenarioListPath, ScenarioListTab, GW2DssMin, GW2DssMax, hdr=True)\n",
    "gw2dss_names = []\n",
    "for i in range(len(gw2dssname)):\n",
    "    gw2dss_names.append(gw2dssname[i][0])\n",
    "gw2dss_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903558b7-1a87-4911-ba9a-2f7ac847da9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexhdr, index_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, IndexMin, IndexMax, hdr=True)\n",
    "index_names = []\n",
    "for i in range(len(index_name)):\n",
    "    index_names.append(index_name[i][0])\n",
    "index_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26379-84b3-425a-a65e-2b6e6852e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "studyhdr, study_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, NameMin, NameMax, hdr=True)\n",
    "study_names = []\n",
    "for i in range(len(study_name)):\n",
    "    study_names.append(study_name[i][0])\n",
    "study_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0c016-d82e-4927-b944-f09db7b2f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirhdr, dir_name = cu.read_from_excel(ScenarioListPath, ScenarioListTab, DirMin, DirMax, hdr=True)\n",
    "dir_names = []\n",
    "for i in range(len(dir_name)):\n",
    "    dir_names.append(dir_name[i][0])\n",
    "dir_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456c0e6-61d8-427b-8d45-1814053c4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "starthdr, start_date = cu.read_from_excel(ScenarioListPath, ScenarioListTab, StartMin, StartMax, hdr=True)\n",
    "start_dates = []\n",
    "for i in range(len(start_date)):\n",
    "    start_dates.append(start_date[i][0])\n",
    "datetime_start_dates = pd.to_datetime(start_dates)\n",
    "# turns out that dss reading library wands a dt datetime, not pd datetime\n",
    "dt_datetime_start_dates = [dt.to_pydatetime() for dt in datetime_start_dates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a887b-829e-4773-bb8c-60e8d6ff35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "endhdr, end_date = cu.read_from_excel(ScenarioListPath, ScenarioListTab, EndMin, EndMax, hdr=True)\n",
    "end_dates = []\n",
    "for i in range(len(end_date)):\n",
    "    end_dates.append(end_date[i][0])\n",
    "# turns out that dss reading library wands a dt datetime, not pd datetime\n",
    "datetime_end_dates = pd.to_datetime(end_dates)\n",
    "dt_datetime_end_dates = [dt.to_pydatetime() for dt in datetime_end_dates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b72d5a-0f63-4a97-958b-00ada84e768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_datetime = min(dt_datetime_start_dates)\n",
    "print('Min time: ')\n",
    "print(min_datetime)\n",
    "max_datetime = max(dt_datetime_end_dates)\n",
    "print('Max time: ')\n",
    "print(max_datetime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19277cc0585f1d94",
   "metadata": {},
   "source": [
    "## Read variables list (not necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3f15cd87dd616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T01:54:08.182311Z",
     "start_time": "2024-04-09T01:54:08.163835Z"
    }
   },
   "outputs": [],
   "source": [
    "# get vars\n",
    "hdr, vars = cu.read_from_excel(VarListPath, VarListTab,VarMin,VarMax,hdr=True)\n",
    "gw1var_df = pd.DataFrame(data=vars, columns=hdr)\n",
    "gw1var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a21684-0557-41e0-b4d2-5ea587fafd5d",
   "metadata": {},
   "source": [
    "## Read the compund data from CSV to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ed99e-35fb-4008-8692-a032f60e8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe from CSV\n",
    "print('Reading ' + DataOutPath)\n",
    "gw1_df, gw1dss_names = read_in_df(DataOutPath,GW1DssNamesOutPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987e457-e886-41f8-a3ae-8261d1e651bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gw1dss_names:\")\n",
    "gw1dss_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acba19-3ddf-4622-9d15-321557ec14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gw1_df:\")\n",
    "gw1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae331c-eeb9-47ad-93a1-b8c6d2b5f7c0",
   "metadata": {},
   "source": [
    "## Drop the LT:E999 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fbcf99-2ce7-4b04-aa0a-3264fa5674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~gw1_df.columns.to_frame().apply(lambda col: col.astype(str).str.contains('LT:E999')).any(axis=1)\n",
    "gw1_df = gw1_df.loc[:, mask.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764c707-1103-4901-ba5d-49a7a9c63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"new gw1_df:\")\n",
    "gw1_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fea1da-0d20-48f0-a906-8a7e61397676",
   "metadata": {},
   "source": [
    "## Add water year column to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e7d4ee-c312-4a0f-a89f-113641fb5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_water_year_column(df):\n",
    "    df_copy = df.copy().sort_index()\n",
    "    df_copy['Date'] = pd.to_datetime(df_copy.index)\n",
    "    df_copy.loc[:, 'Year'] = df_copy['Date'].dt.year\n",
    "    df_copy.loc[:, 'Month'] = df_copy['Date'].dt.month\n",
    "    df_copy.loc[:, 'WaterYear'] = np.where(df_copy['Month'] >= 10, df_copy['Year'] + 1, df_copy['Year'])\n",
    "    return df_copy.drop([\"Date\", \"Year\", \"Month\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa138763-4196-4fe7-8c48-f97f5e277d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gw1_df = add_water_year_column(gw1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723b11a-2446-437c-8d5c-060fbfb37d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gw1_df with water year column:\")\n",
    "gw1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ae3ad-5839-43cf-a557-638d77f26a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(\".\")\n",
    "\n",
    "wresl_path = os.path.join(base_dir, \"CalSim3GWregionIndex.wresl\")\n",
    "wba_area_path = os.path.join(base_dir, \"CalSim3_WBA.csv\")\n",
    "\n",
    "# Read the area data (CSV)\n",
    "wba_df = pd.read_csv(wba_area_path)\n",
    "\n",
    "# Check what columns are present\n",
    "print(\"Columns in WBA Area CSV:\", wba_df.columns)\n",
    "\n",
    "# Preview key columns\n",
    "print(wba_df[['fid', 'GIS_Acres']].head())\n",
    "\n",
    "# === Step 3: Parse WRESL File to Map SRxx to WBAxx ===\n",
    "\n",
    "with open(wresl_path, 'r') as f:\n",
    "    wresl_lines = f.readlines()\n",
    "\n",
    "# Extract SRxx → WBAxx or DETAW\n",
    "sr_to_wba_map = {}\n",
    "for line in wresl_lines:\n",
    "    # Match standard WBA format: indxWBA_XX = SRYY\n",
    "    match = re.match(r'\\s*indxWBA_(\\d+)\\s*=\\s*(SR\\d+)', line)\n",
    "    if match:\n",
    "        wba_num, sr_num = match.groups()\n",
    "        sr_to_wba_map[sr_num] = f'WBA{wba_num}'\n",
    "    else:\n",
    "        # Match DETAW format: indxDETAW = SRYY\n",
    "        match_detaw = re.match(r'\\s*indxDETAW\\s*=\\s*(SR\\d+)', line)\n",
    "        if match_detaw:\n",
    "            sr_num = match_detaw.group(1)\n",
    "            sr_to_wba_map[sr_num] = 'DETAW'\n",
    "\n",
    "# Convert the mapping to a DataFrame for easier viewing\n",
    "mapping_df = pd.DataFrame(list(sr_to_wba_map.items()), columns=['SR_number', 'WBA_name'])\n",
    "\n",
    "# Preview result\n",
    "print(\"\\n=== SR to WBA Mapping Preview ===\")\n",
    "print(mapping_df.head())\n",
    "\n",
    "# Save mapping to CSV if needed\n",
    "# mapping_df.to_csv(\"sr_to_wba_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd86ff6-2b57-40fc-aa8c-c7b2c01553bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(\".\")\n",
    "\n",
    "wresl_path = os.path.join(base_dir, \"CalSim3GWregionIndex.wresl\")\n",
    "wba_csv_path = os.path.join(base_dir, \"CalSim3_WBA.csv\")\n",
    "with open(wresl_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Parse mappings\n",
    "sr_to_wba_map = {}\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "\n",
    "    # Handle standard: define indxWBA_2 {value 1 }\n",
    "    match_wba = re.match(r'define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    if match_wba:\n",
    "        wba_id, sr_num = match_wba.groups()\n",
    "        sr_key = f\"SR{int(sr_num):02d}\"       # e.g. 1 → SR01\n",
    "        wba_value = f\"WBA{wba_id}\"            # e.g. 2 → WBA2\n",
    "        sr_to_wba_map[sr_key] = wba_value\n",
    "        continue\n",
    "\n",
    "    # Handle special case: define indxDETAW {value 42 }\n",
    "    match_detaw = re.match(r'define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    if match_detaw:\n",
    "        sr_num = match_detaw.group(1)\n",
    "        sr_key = f\"SR{int(sr_num):02d}\"       # e.g. 42 → SR42\n",
    "        sr_to_wba_map[sr_key] = \"DETAW\"\n",
    "\n",
    "# Convert to DataFrame\n",
    "mapping_df = pd.DataFrame(list(sr_to_wba_map.items()), columns=[\"SR_number\", \"WBA_name\"])\n",
    "print(mapping_df.tail(10))  # check the DETAW row\n",
    "\n",
    "# Optionally save\n",
    "# mapping_df.to_csv(\"sr_to_wba_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f3004-443e-4069-bd8e-afc80de3d9fb",
   "metadata": {},
   "source": [
    "## End of initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b9a43-3bb2-4d0f-93cd-c2836a9278ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done Initializing!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedad83-c48d-42b7-aa85-967e6e2c7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(\".\")\n",
    "\n",
    "wresl_path = os.path.join(base_dir, \"CalSim3GWregionIndex.wresl\")\n",
    "wba_csv_path = os.path.join(base_dir, \"CalSim3_WBA.csv\")\n",
    "\n",
    "with open(wresl_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "mapping_records = []\n",
    "\n",
    "i = 1\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    # print(\"line \" + str(i) + \":\")\n",
    "    # print(line)\n",
    "    match_wba = re.match(r'define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    # print(\"match_wba \" + str(i) + \":\")\n",
    "    # print(match_wba)\n",
    "    \n",
    "    if match_wba:\n",
    "        wba_id, sr_num = match_wba.groups()\n",
    "        # print(\"wba_id \" + str(i) + \":\")\n",
    "        # print(wba_id)\n",
    "        # print(\"sr_num \" + str(i) + \":\")\n",
    "        # print(sr_num)\n",
    "        mapping_records.append({\n",
    "            \"Subregion_ID\": f\"SR{int(sr_num):02d}\",\n",
    "            \"WBA_ID\": f\"WBA{wba_id}\"\n",
    "        })\n",
    "        continue\n",
    "    i = i + 1\n",
    "    match_detaw = re.match(r'define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "    if match_detaw:\n",
    "        sr_num = match_detaw.group(1)\n",
    "        mapping_records.append({\n",
    "            \"Subregion_ID\": f\"SR{int(sr_num):02d}\",\n",
    "            \"WBA_ID\": \"DETAW\"\n",
    "        })\n",
    "\n",
    "# print(\"mapping_records:\")\n",
    "# print(mapping_records)\n",
    "\n",
    "wresl_df = pd.DataFrame(mapping_records).sort_values(\"Subregion_ID\")\n",
    "display(wresl_df)\n",
    "\n",
    "output_csv_path = os.path.join(base_dir, \"groundwater_wresl_mapping.csv\")\n",
    "wresl_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"Saved WRESL mapping to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a35f52-c070-411d-a86d-9f017a6b7f8f",
   "metadata": {},
   "source": [
    "## Monthly, Annual Data and Trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579bec28-415e-4376-a3b5-478e582b29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup base paths ===\n",
    "base_dir = os.path.abspath(\".\")\n",
    "wresl_path = os.path.join(base_dir, \"CalSim3GWregionIndex.wresl\")\n",
    "wba_csv_path = os.path.join(base_dir, \"CalSim3_WBA.csv\")\n",
    "#GroupDataDirPath = os.path.join(base_dir, \"output\")\n",
    "\n",
    "# === Load mapping files ===\n",
    "wba_df = pd.read_csv(wba_csv_path)\n",
    "sr_to_fid = {f\"SR{int(fid):02d}\": fid for fid in wba_df['fid']}\n",
    "fid_to_acres = dict(zip(wba_df['fid'], wba_df['GIS_Acres']))\n",
    "\n",
    "sr_to_wba = {}\n",
    "with open(wresl_path, \"r\") as file:\n",
    "    i = 1\n",
    "    for line in file:\n",
    "\n",
    "        # print(\"line \" + str(i) + \":\")\n",
    "        # print(line)\n",
    "        \n",
    "        match_wba = re.match(r'define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "        # print(\"match_wba \" + str(i) + \":\")\n",
    "        # print(match_wba)\n",
    "    \n",
    "        if match_wba:\n",
    "            wba_id, sr_num = match_wba.groups()\n",
    "            # print(\"wba_id \" + str(i) + \":\")\n",
    "            # print(wba_id)\n",
    "            # print(\"sr_num \" + str(i) + \":\")\n",
    "            # print(sr_num)\n",
    "            sr_to_wba[f\"SR{int(sr_num):02d}\"] = f\"WBA{wba_id}\"\n",
    "        else:\n",
    "            match_detaw = re.match(r'define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}', line)\n",
    "            if match_detaw:\n",
    "                sr_num = match_detaw.group(1)\n",
    "                sr_to_wba[f\"SR{int(sr_num):02d}\"] = \"DETAW\"\n",
    "        i = i + 1\n",
    "        \n",
    "# print(\"sr_to_wba:\")\n",
    "# print(sr_to_wba)\n",
    "\n",
    "# === Process data ===\n",
    "converted_data = {}\n",
    "monthly_trend = {}\n",
    "annual_trend = {}\n",
    "scenario_drops = {}\n",
    "start_year = 1960\n",
    "\n",
    "# print(\"gw1_df:\")\n",
    "# print(gw1_df)\n",
    "\n",
    "for col in gw1_df.columns:\n",
    "    if not isinstance(col, tuple) or len(col) < 5:\n",
    "        continue\n",
    "\n",
    "    model, var_tag, var_type, timestep, unit = col[:5]\n",
    "    if not re.match(r'^SR\\d+:TOT_s\\d{4}$', var_tag):\n",
    "        continue\n",
    "\n",
    "    sr, rest = var_tag.split(\":\")\n",
    "    scenario_raw = rest.split(\"_s\")[-1]\n",
    "    scenario = f\"s{int(scenario_raw):04d}\"\n",
    "    if sr not in sr_to_wba:\n",
    "        continue\n",
    "\n",
    "    wba = sr_to_wba[sr]\n",
    "    fid = sr_to_fid.get(sr)\n",
    "    area = fid_to_acres.get(fid, None)\n",
    "    if area is None or area == 0:\n",
    "        continue\n",
    "\n",
    "    new_var_tag = f\"{wba}_{scenario}\"\n",
    "    values = gw1_df[col]\n",
    "    values = values[values.index >= pd.Timestamp(f\"{start_year}-01-01\")]\n",
    "    values_ft = (values / area) * 1000\n",
    "\n",
    "    drop_indices = np.where(values_ft < 0)[0]\n",
    "    if len(drop_indices) > 0:\n",
    "        cutoff_idx = drop_indices[0]\n",
    "        drop_range = (values_ft.index[cutoff_idx].date(), values_ft.index[-1].date())\n",
    "        scenario_drops[scenario] = drop_range\n",
    "        values_ft = values_ft.iloc[:cutoff_idx]\n",
    "\n",
    "    new_col = (model, new_var_tag, var_type, timestep, \"FT\")\n",
    "    converted_data[new_col] = values_ft\n",
    "\n",
    "    x_m = np.arange(len(values_ft))\n",
    "    slope_m = np.polyfit(x_m, values_ft.values, 1)[0] if len(values_ft.dropna()) >= 2 else np.nan\n",
    "    monthly_trend.setdefault(scenario, {})[f\"{wba}:TOT\"] = slope_m\n",
    "\n",
    "    df_annual = values_ft.resample(\"YE\").mean()\n",
    "    df_annual.index = df_annual.index.year\n",
    "    x_a = np.arange(len(df_annual))\n",
    "    slope_a = np.polyfit(x_a, df_annual.values, 1)[0] if len(df_annual.dropna()) >= 2 else np.nan\n",
    "    annual_trend.setdefault(scenario, {})[f\"{wba}:TOT\"] = slope_a\n",
    "\n",
    "# print(\"annual_trend:\")\n",
    "# print(annual_trend)\n",
    "\n",
    "# === Save processed monthly data ===\n",
    "gw1_df_filtered = pd.concat(converted_data, axis=1)\n",
    "gw1_df_filtered.columns = pd.MultiIndex.from_tuples(\n",
    "    gw1_df_filtered.columns, names=[\"Model\", \"Variable\", \"Type\", \"Timestep\", \"Unit\"]\n",
    ")\n",
    "gw1_df_filtered = gw1_df_filtered[gw1_df_filtered.index >= pd.Timestamp(\"1960-01-01\")]\n",
    "monthly_csv_path = os.path.join(GroupDataDirPath, f\"GroundWater_DataMonthly.csv\")\n",
    "gw1_df_filtered.to_csv(monthly_csv_path)\n",
    "\n",
    "# === Save processed annual data ===\n",
    "annual_df = gw1_df_filtered.resample(\"YE\").mean()\n",
    "annual_df.index = annual_df.index.year\n",
    "annual_df.columns = pd.MultiIndex.from_tuples(\n",
    "    [(model, var, typ, \"1YEAR\", unit) for model, var, typ, _, unit in annual_df.columns],\n",
    "    names=[\"Model\", \"Variable\", \"Type\", \"Timestep\", \"Unit\"]\n",
    ")\n",
    "annual_csv_path = os.path.join(GroupDataDirPath, f\"GroundWater_DataAnnual.csv\")\n",
    "annual_df.to_csv(annual_csv_path)\n",
    "\n",
    "# === Save monthly trend slopes ===\n",
    "monthly_df = pd.DataFrame(monthly_trend).T.sort_index()\n",
    "monthly_df.columns.name = \"Variable\"\n",
    "monthly_units = pd.DataFrame([[\"FT/MON\"] * monthly_df.shape[1]], columns=monthly_df.columns)\n",
    "monthly_combined = pd.concat([monthly_units, monthly_df], ignore_index=True)\n",
    "monthly_combined.index = [\"Unit\"] + list(monthly_df.index)\n",
    "monthly_trend_path = os.path.join(GroupDataDirPath, f\"GroundWater_TrendMonthly.csv\")\n",
    "monthly_combined.to_csv(monthly_trend_path)\n",
    "\n",
    "# === Save annual trend slopes ===\n",
    "annual_df_trend = pd.DataFrame(annual_trend).T.sort_index()\n",
    "annual_df_trend.columns.name = \"Variable\"\n",
    "\n",
    "# print(\"annual_df_trend:\")\n",
    "# print(annual_df_trend)\n",
    "\n",
    "annual_units = pd.DataFrame([[\"FT/YEAR\"] * annual_df_trend.shape[1]], columns=annual_df_trend.columns)\n",
    "annual_combined = pd.concat([annual_units, annual_df_trend], ignore_index=True)\n",
    "annual_combined.index = [\"Unit\"] + list(annual_df_trend.index)\n",
    "annual_trend_path = os.path.join(GroupDataDirPath, f\"GroundWater_TrendAnnual.csv\")\n",
    "annual_combined.to_csv(annual_trend_path)\n",
    "\n",
    "# === Print drop summary ===\n",
    "print(\"\\n--- Summary of Dropped Data by Scenario ---\")\n",
    "for scen, (start, end) in scenario_drops.items():\n",
    "    print(f\"Scenario {scen} dropped from {start} to {end}\")\n",
    "\n",
    "# === Calculate differences from baseline ===\n",
    "baseline_scenario = \"s0011\"\n",
    "diff_data = {}\n",
    "\n",
    "for col in gw1_df_filtered.columns:\n",
    "    model, var, typ, timestep, unit = col\n",
    "    if baseline_scenario not in var:\n",
    "        scenario_code = var.split(\"_\")[-1]\n",
    "        base_var = var.replace(f\"_{scenario_code}\", f\"_{baseline_scenario}\")\n",
    "        baseline_col = (model, base_var, typ, timestep, unit)\n",
    "\n",
    "        if baseline_col in gw1_df_filtered.columns:\n",
    "            series_other = gw1_df_filtered[col]\n",
    "            series_base = gw1_df_filtered[baseline_col]\n",
    "            valid_index = series_other.dropna().index.intersection(series_base.dropna().index)\n",
    "            if len(valid_index) > 0:\n",
    "                avg_diff = (series_other.loc[valid_index] - series_base.loc[valid_index]).mean()\n",
    "                diff_key = var.replace(f\"_{scenario_code}\", \"\")\n",
    "                diff_data.setdefault(scenario_code, {})[diff_key] = avg_diff\n",
    "\n",
    "# === Save AvgDiff output ===\n",
    "diff_df = pd.DataFrame(diff_data).T.sort_index()\n",
    "diff_df.columns.name = \"Variable\"\n",
    "\n",
    "note_row = pd.DataFrame(\n",
    "    [[\"Average difference: scenario X minus scenario s0011\"] + [\"\"] * (diff_df.shape[1] - 1)],\n",
    "    columns=diff_df.columns\n",
    ")\n",
    "unit_row = pd.DataFrame([[\"FT\"] * diff_df.shape[1]], columns=diff_df.columns)\n",
    "diff_combined = pd.concat([note_row, unit_row, diff_df], ignore_index=True)\n",
    "diff_combined.index = [\"Note\", \"Unit\"] + list(diff_df.index)\n",
    "\n",
    "diff_output_path = os.path.join(GroupDataDirPath, f\"GroundWater_AvgDiff.csv\")\n",
    "diff_combined.to_csv(diff_output_path)\n",
    "\n",
    "print(f\"\\n✓ All files saved to:\\n{GroupDataDirPath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f541c4-06f5-4963-8bad-2b86d63954b8",
   "metadata": {},
   "source": [
    "# Plot histograms to find a natural break in the trends of baseline scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f4763b-4df4-496b-aa44-cd0fac9015af",
   "metadata": {},
   "source": [
    "## Specify baseline index, quantiles and number of bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f9ba8-bc17-4e7b-adcf-b5e9d2e7a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_index = \"s0011\"\n",
    "lQuant = 0.05\n",
    "hQuant = 0.4\n",
    "nBins = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2db3f-3e27-486a-b9bc-6c8eb5f0df22",
   "metadata": {},
   "source": [
    "## Plot baseline trend histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08128a46-6551-4262-b209-fc1411e0f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the row\n",
    "baseline_data = annual_df_trend.loc[baseline_index]\n",
    "# print(\"Baseline Data:\")\n",
    "# print(baseline_data)\n",
    "# Plot histogram\n",
    "bins = nBins\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts, bin_edges, _ = plt.hist(baseline_data, bins=bins, edgecolor='black')# Add more x-axis ticks using bin edges\n",
    "plt.xticks(np.round(bin_edges, 3), rotation=45)  # Round for readability\n",
    "plt.title('Histogram of Annual Trends for ' + baseline_index)\n",
    "plt.xlabel('Slope Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df9df3-8e14-4d1b-a6b1-0b9c3234c06e",
   "metadata": {},
   "source": [
    "## Plot clipped baseline trend histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f8159-5bb9-478c-9083-1af752699b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quantiles\n",
    "lVal, hVal = np.quantile(baseline_data.values, [lQuant, hQuant])\n",
    "\n",
    "# clip data\n",
    "clipped_data = baseline_data.values.clip(lVal, hVal)\n",
    "# print(\"Clipped Data:\")\n",
    "# print(clipped_data)\n",
    "\n",
    "# Plot histogram\n",
    "bins = nBins\n",
    "plt.figure(figsize=(10, 6))\n",
    "counts, bin_edges, _ = plt.hist(clipped_data, bins=bins, edgecolor='black')# Add more x-axis ticks using bin edges\n",
    "plt.xticks(np.round(bin_edges, 3), rotation=45)  # Round for readability\n",
    "plt.title('Histogram of Annual Trends for ' + baseline_index + \" (after clipping data)\")\n",
    "plt.xlabel('Slope Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552b0483-4dfa-499b-a142-f7ca639c63d9",
   "metadata": {},
   "source": [
    "## Notes: Where to put the break? What quantiles to use to distinguish moderate from severe? Propose: -0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ab60b9-f1d8-460d-b944-71440b0c5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_calsim_base_path(start_path, sibling_name=\"CalSim3_Model_Runs\"):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    dsp_root = os.path.dirname(os.path.dirname(current_path))  # notebooks → coeqwal → DSP\n",
    "    candidate = os.path.join(dsp_root, sibling_name)\n",
    "    if os.path.isdir(candidate):\n",
    "        return candidate\n",
    "    raise FileNotFoundError(f\"{sibling_name} not found under {dsp_root}\")\n",
    "\n",
    "dsp_root = os.path.dirname(os.path.dirname(os.path.abspath(\".\")))\n",
    "csv_path = os.path.join(\n",
    "    dsp_root,\n",
    "    \"CalSim3_Model_Runs\",\n",
    "    \"Scenarios\",\n",
    "    \"Group_Data_Extraction\",\n",
    "    \"GroundWater_DataMonthly.csv\"\n",
    ")\n",
    "\n",
    "gw1_df_filtered = pd.read_csv(\n",
    "    csv_path,\n",
    "    header=[0, 1, 2, 3, 4],\n",
    "    index_col=0,\n",
    "    parse_dates=True\n",
    ")\n",
    "\n",
    "end_year_override = {\"s0006\", \"s0007\", \"s0008\", \"s0009\", \"s0010\"}\n",
    "drop_threshold = 1000\n",
    "start_year = 1960\n",
    "\n",
    "plot_output_dir = os.path.join(\n",
    "    dsp_root,\n",
    "    \"CalSim3_Model_Runs\",\n",
    "    \"Scenarios\",\n",
    "    \"Performance_Metrics\",\n",
    "    \"Groundwater\"\n",
    ")\n",
    "os.makedirs(plot_output_dir, exist_ok=True)\n",
    "\n",
    "for col in gw1_df_filtered.columns:\n",
    "    model, var, typ, timestep, unit = col\n",
    "    if not re.match(r'^(WBA\\d+)_s\\d{4}$', var):\n",
    "        continue\n",
    "\n",
    "    wba_id, scenario = var.split(\"_\")\n",
    "    print(\"wba_id: \" + str(wba_id) + \", scenario: \" + scenario)\n",
    "    ts = gw1_df_filtered[col]\n",
    "    ts = ts[ts.index >= pd.Timestamp(f\"{start_year}-01-01\")].dropna()\n",
    "\n",
    "    if scenario in end_year_override:\n",
    "        ts = ts[ts.index < pd.Timestamp(\"2016-01-01\")]\n",
    "        drop_year = 2015\n",
    "    else:\n",
    "        diffs = ts.diff()\n",
    "        drop_indices = diffs[diffs < -drop_threshold].index\n",
    "        if not drop_indices.empty:\n",
    "            cutoff_idx = drop_indices[0]\n",
    "            ts = ts[ts.index <= cutoff_idx]\n",
    "            drop_year = cutoff_idx.year\n",
    "        else:\n",
    "            drop_year = 2021\n",
    "\n",
    "    if len(ts) < 2:\n",
    "        continue  # skip if not enough data\n",
    "\n",
    "    x = (ts.index - ts.index[0]).days / 365.25\n",
    "    y = ts.values\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    trend = slope * x + intercept\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(ts.index, y, label=\"Observed\", marker='o')\n",
    "    plt.plot(ts.index, trend, linestyle='--', label=f\"Trend (slope={slope:.6f})\")\n",
    "    plt.title(f\"{wba_id} under {scenario} (end year: {drop_year})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Groundwater Storage (FT)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_name = f\"{scenario}_{wba_id}.png\"\n",
    "    save_path = os.path.join(plot_output_dir, save_name)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show() \n",
    "    plt.close()\n",
    "    print(f\"✓ Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957c827-266f-4d6a-8450-849b2722abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plot_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87335a0c-7cf7-4c4f-b5d3-edf39aa200b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = \"SR10:TOT\"\n",
    "\n",
    "matching_cols = [col for col in gw1_df.columns\n",
    "                 if isinstance(col, tuple) and col[1].startswith(target_variable)]\n",
    "\n",
    "df_2015 = gw1_df[gw1_df.index.year == 2015]\n",
    "\n",
    "result_df = df_2015[matching_cols]\n",
    "\n",
    "result_df.columns = [f\"{col[1]}\" for col in result_df.columns]\n",
    "\n",
    "print(\"Groundwater variable values for\", target_variable, \"in 2015:\\n\")\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aaced6-5710-4f3f-b401-58df5b0ce62c",
   "metadata": {},
   "source": [
    "## Tier Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f8ab8-651b-4b3e-9083-720185f2cb8e",
   "metadata": {},
   "source": [
    "## Find start point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dfd66e-b845-4e29-afdc-d07774415c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "start_date = pd.Timestamp(\"1990-01-01\")\n",
    "end_date = pd.Timestamp(\"2000-12-31\")\n",
    "\n",
    "min_date_records = []\n",
    "\n",
    "for col in gw1_df_filtered.columns:\n",
    "    model, var, typ, timestep, unit = col\n",
    "    if not re.match(r'^WBA\\d+_s\\d{4}$', var):\n",
    "        continue\n",
    "\n",
    "    wba_id, scenario = var.split(\"_\")\n",
    "    ts = gw1_df_filtered[col]\n",
    "    ts = ts[(ts.index >= start_date) & (ts.index <= end_date)].dropna()\n",
    "\n",
    "    if not ts.empty:\n",
    "        min_idx = ts.idxmin()\n",
    "        min_date_records.append(min_idx)\n",
    "\n",
    "date_counts = Counter(min_date_records)\n",
    "mode_date, mode_count = date_counts.most_common(1)[0]\n",
    "\n",
    "print(f\"✓ Found {len(min_date_records)} minimum dates from 1990–2000.\")\n",
    "print(f\" The most frequent minimum date (mode) is: {mode_date} ({mode_count} occurrences)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0eb9ab-514c-408d-9f60-a704f6f94b59",
   "metadata": {},
   "source": [
    "### Function to calculate slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3713b86-8cd7-4f27-8ba8-880ff5e5e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scenario_slope_df(gw1_df_filtered, scenario, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Computes slope for each WBA under the specified scenario over an optional date range.\n",
    "\n",
    "    Parameters:\n",
    "        gw1_df_filtered (pd.DataFrame): Time series data with MultiIndex columns.\n",
    "        scenario (str): Scenario number as a string (e.g., '11').\n",
    "        start_date (str or pd.Timestamp, optional): Start of range.\n",
    "        end_date (str or pd.Timestamp, optional): End of range.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns ['scenario', 'WBA', 'slope'].\n",
    "    \"\"\"\n",
    "    result_records = []\n",
    "\n",
    "    for col in gw1_df_filtered.columns:\n",
    "        model, var, typ, timestep, unit = col\n",
    "        if not var.endswith(f\"s{int(scenario):04d}\"):\n",
    "            continue\n",
    "\n",
    "        wba = var.replace(f\"_s{int(scenario):04d}\", \"\")\n",
    "        ts = gw1_df_filtered[col]\n",
    "\n",
    "        # Filter by date range\n",
    "        if start_date:\n",
    "            ts = ts[ts.index >= pd.to_datetime(start_date)]\n",
    "        if end_date:\n",
    "            ts = ts[ts.index <= pd.to_datetime(end_date)]\n",
    "\n",
    "        ts = ts.dropna()\n",
    "        if len(ts) < 2:\n",
    "            slope = np.nan\n",
    "        else:\n",
    "            x = np.arange(len(ts))\n",
    "            y = ts.values\n",
    "            slope, _ = np.polyfit(x, y, 1)\n",
    "\n",
    "        result_records.append({\n",
    "            \"scenario\": f\"s{int(scenario):04d}\",\n",
    "            \"WBA\": wba,\n",
    "            \"slope\": slope\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(result_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ec7a4-3a9c-4fdb-9541-98d3327b970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute slopes for scenario 11 between 1990 and 2000\n",
    "slope_df = compute_scenario_slope_df(gw1_df_filtered, scenario=\"11\", start_date=\"1992-09-30\")\n",
    "print(slope_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa94fd-c294-4ccf-ba1a-93bd3fc30e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Corrected function to find top-level CalSim3_Model_Runs ===\n",
    "def find_calsim_base_path(start_path, target_folder=\"CalSim3_Model_Runs\"):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        parent = os.path.dirname(current_path)\n",
    "        candidate = os.path.join(parent, target_folder)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        if parent == current_path:\n",
    "            raise FileNotFoundError(f\"{target_folder} not found above {start_path}\")\n",
    "        current_path = parent\n",
    "\n",
    "# === Main function ===\n",
    "def assign_tiers(base_dir, severe_decline_threshold=-0.015):\n",
    "    # Find CalSim3_Model_Runs path\n",
    "    calsim_base_path = find_calsim_base_path(base_dir)\n",
    "\n",
    "    # Set paths\n",
    "    group_data_path = os.path.join(\n",
    "        calsim_base_path, \"Scenarios\", \"Group_Data_Extraction\"\n",
    "    )\n",
    "    trend_annual_path = os.path.join(group_data_path, \"GroundWater_TrendAnnual.csv\")\n",
    "    avg_diff_path = os.path.join(group_data_path, \"GroundWater_AvgDiff.csv\")\n",
    "    tier_output_path = os.path.join(group_data_path, \"GroundwaterTier_Annual.csv\")\n",
    "\n",
    "    wresl_path = os.path.join(base_dir, \"CalSim3GWregionIndex.wresl\")\n",
    "    wba_csv_path = os.path.join(base_dir, \"CalSim3_WBA.csv\")\n",
    "\n",
    "    # Load metadata\n",
    "    wba_df = pd.read_csv(wba_csv_path)\n",
    "    sr_to_fid = {f\"SR{int(fid):02d}\": fid for fid in wba_df[\"fid\"]}\n",
    "    fid_to_acres = dict(zip(wba_df[\"fid\"], wba_df[\"GIS_Acres\"]))\n",
    "\n",
    "    sr_to_wba = {}\n",
    "    with open(wresl_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            match_wba = re.match(r\"define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}\", line)\n",
    "            if match_wba:\n",
    "                wba_id, sr_num = match_wba.groups()\n",
    "                sr_to_wba[f\"SR{int(sr_num):02d}\"] = f\"WBA{wba_id}\"\n",
    "            else:\n",
    "                match_detaw = re.match(r\"define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}\", line)\n",
    "                if match_detaw:\n",
    "                    sr_num = match_detaw.group(1)\n",
    "                    sr_to_wba[f\"SR{int(sr_num):02d}\"] = \"DETAW\"\n",
    "\n",
    "    # Load data\n",
    "    trend_df = pd.read_csv(trend_annual_path, index_col=0)\n",
    "    diff_df = pd.read_csv(avg_diff_path, index_col=0)\n",
    "\n",
    "    # Clean and format\n",
    "    trend_df = trend_df.drop(index=[\"Unit\"], errors=\"ignore\")\n",
    "    diff_df = diff_df.drop(index=[\"Note\", \"Unit\"], errors=\"ignore\")\n",
    "\n",
    "    trend_df = trend_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    diff_df = diff_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    trend_df.columns = [col.replace(\":TOT\", \"\") for col in trend_df.columns]\n",
    "    diff_df.columns = [col.strip() for col in diff_df.columns]\n",
    "\n",
    "    trend_df.index.name = \"Scenario\"\n",
    "    diff_df.index.name = \"Scenario\"\n",
    "\n",
    "    # Tier assignment\n",
    "    baseline_scenario = \"s0011\"\n",
    "    wba_cols = trend_df.columns.tolist()\n",
    "    tier_matrix = pd.DataFrame(index=trend_df.index, columns=wba_cols)\n",
    "\n",
    "    for wba_col in wba_cols:\n",
    "        if baseline_scenario not in trend_df.index:\n",
    "            continue\n",
    "        baseline_slope = trend_df.loc[baseline_scenario, wba_col]\n",
    "\n",
    "        for scenario in trend_df.index:\n",
    "            if scenario == baseline_scenario:\n",
    "                tier_matrix.loc[scenario, wba_col] = 0\n",
    "                continue\n",
    "\n",
    "            trend = trend_df.loc[scenario, wba_col]\n",
    "            diff = diff_df.loc[scenario, wba_col] if wba_col in diff_df.columns else np.nan\n",
    "\n",
    "            if pd.isna(trend) or pd.isna(diff) or pd.isna(baseline_slope):\n",
    "                tier = np.nan\n",
    "            elif trend >= 0:\n",
    "                tier = 1 if diff >= 0 else 2\n",
    "            elif trend >= severe_decline_threshold:\n",
    "                tier = 3\n",
    "            else:\n",
    "                tier = 4\n",
    "\n",
    "            tier_matrix.loc[scenario, wba_col] = tier\n",
    "\n",
    "    # Save output\n",
    "    tier_matrix.to_csv(tier_output_path)\n",
    "    print(f\"✓ Tier assignment saved to {tier_output_path}\")\n",
    "    print(f\"✓ Used severe_decline_threshold = {severe_decline_threshold}\")\n",
    "    return tier_matrix\n",
    "\n",
    "# === Run script directly ===\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = os.path.abspath(\".\")\n",
    "    assign_tiers(base_dir, severe_decline_threshold=-0.015)\n",
    "\n",
    "tier_df = assign_tiers(base_dir, severe_decline_threshold=-0.015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd7152-6f8b-4610-a5f6-75e7a746627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Corrected function to find top-level CalSim3_Model_Runs ===\n",
    "def find_calsim_base_path(start_path, target_folder=\"CalSim3_Model_Runs\"):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        parent = os.path.dirname(current_path)\n",
    "        candidate = os.path.join(parent, target_folder)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        if parent == current_path:\n",
    "            raise FileNotFoundError(f\"{target_folder} not found above {start_path}\")\n",
    "        current_path = parent\n",
    "\n",
    "# === Main function ===\n",
    "def assign_monthly_tiers(base_dir, severe_decline_threshold=-0.015):\n",
    "    # Find CalSim3_Model_Runs path\n",
    "    calsim_base_path = find_calsim_base_path(base_dir)\n",
    "\n",
    "    # Set paths\n",
    "    group_data_path = os.path.join(\n",
    "        calsim_base_path, \"Scenarios\", \"Group_Data_Extraction\"\n",
    "    )\n",
    "    trend_monthly_path = os.path.join(group_data_path, \"GroundWater_TrendMonthly.csv\")\n",
    "    avg_diff_path = os.path.join(group_data_path, \"GroundWater_AvgDiff.csv\")\n",
    "    tier_output_path = os.path.join(group_data_path, \"GroundwaterTier_Monthly.csv\")\n",
    "\n",
    "    wresl_path = os.path.join(base_dir, \"CalSim3GWregionIndex.wresl\")\n",
    "    wba_csv_path = os.path.join(base_dir, \"CalSim3_WBA.csv\")\n",
    "\n",
    "    # Load metadata\n",
    "    wba_df = pd.read_csv(wba_csv_path)\n",
    "    sr_to_fid = {f\"SR{int(fid):02d}\": fid for fid in wba_df[\"fid\"]}\n",
    "    fid_to_acres = dict(zip(wba_df[\"fid\"], wba_df[\"GIS_Acres\"]))\n",
    "\n",
    "    sr_to_wba = {}\n",
    "    with open(wresl_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            match_wba = re.match(r\"define\\s+indxWBA_([0-9A-Za-z]+)\\s+\\{value\\s+(\\d+)\\s+\\}\", line)\n",
    "            if match_wba:\n",
    "                wba_id, sr_num = match_wba.groups()\n",
    "                sr_to_wba[f\"SR{int(sr_num):02d}\"] = f\"WBA{wba_id}\"\n",
    "            else:\n",
    "                match_detaw = re.match(r\"define\\s+indxDETAW\\s+\\{value\\s+(\\d+)\\s+\\}\", line)\n",
    "                if match_detaw:\n",
    "                    sr_num = match_detaw.group(1)\n",
    "                    sr_to_wba[f\"SR{int(sr_num):02d}\"] = \"DETAW\"\n",
    "\n",
    "    # Load data\n",
    "    trend_df = pd.read_csv(trend_monthly_path, index_col=0)\n",
    "    diff_df = pd.read_csv(avg_diff_path, index_col=0)\n",
    "\n",
    "    # Clean and format\n",
    "    trend_df = trend_df.drop(index=[\"Unit\"], errors=\"ignore\")\n",
    "    diff_df = diff_df.drop(index=[\"Note\", \"Unit\"], errors=\"ignore\")\n",
    "\n",
    "    trend_df = trend_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    diff_df = diff_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    trend_df.columns = [col.replace(\":TOT\", \"\") for col in trend_df.columns]\n",
    "    diff_df.columns = [col.strip() for col in diff_df.columns]\n",
    "\n",
    "    trend_df.index.name = \"Scenario\"\n",
    "    diff_df.index.name = \"Scenario\"\n",
    "\n",
    "    # Tier assignment\n",
    "    baseline_scenario = \"s0011\"\n",
    "    wba_cols = trend_df.columns.tolist()\n",
    "    tier_matrix = pd.DataFrame(index=trend_df.index, columns=wba_cols)\n",
    "\n",
    "    for wba_col in wba_cols:\n",
    "        if baseline_scenario not in trend_df.index:\n",
    "            continue\n",
    "        baseline_slope = trend_df.loc[baseline_scenario, wba_col]\n",
    "\n",
    "        for scenario in trend_df.index:\n",
    "            if scenario == baseline_scenario:\n",
    "                tier_matrix.loc[scenario, wba_col] = 0\n",
    "                continue\n",
    "\n",
    "            trend = trend_df.loc[scenario, wba_col]\n",
    "            diff = diff_df.loc[scenario, wba_col] if wba_col in diff_df.columns else np.nan\n",
    "\n",
    "            if pd.isna(trend) or pd.isna(diff) or pd.isna(baseline_slope):\n",
    "                tier = np.nan\n",
    "            elif trend >= 0:\n",
    "                tier = 1 if diff >= 0 else 2\n",
    "            elif trend >= severe_decline_threshold:\n",
    "                tier = 3\n",
    "            else:\n",
    "                tier = 4\n",
    "\n",
    "            tier_matrix.loc[scenario, wba_col] = tier\n",
    "\n",
    "    # Save output\n",
    "    tier_matrix.to_csv(tier_output_path)\n",
    "    print(f\"✓ Monthly tier assignment saved to {tier_output_path}\")\n",
    "    print(f\"✓ Used severe_decline_threshold = {severe_decline_threshold}\")\n",
    "    return tier_matrix\n",
    "\n",
    "# === Run script directly ===\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = os.path.abspath(\".\")\n",
    "    assign_monthly_tiers(base_dir, severe_decline_threshold=-0.015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849130d-0559-4275-bbc9-ae146c53193f",
   "metadata": {},
   "source": [
    "## Trend Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977063c6-bd72-4d6c-9571-a9f8cc3be05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Helper to find CalSim3_Model_Runs path ===\n",
    "def find_calsim_base_path(start_path, sibling_name=\"CalSim3_Model_Runs\"):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        parent = os.path.dirname(current_path)\n",
    "        candidate = os.path.join(parent, sibling_name)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        if parent == current_path:\n",
    "            raise FileNotFoundError(f\"{sibling_name} not found above {start_path}\")\n",
    "        current_path = parent\n",
    "\n",
    "# === Paths ===\n",
    "base_dir = os.path.abspath(\".\")\n",
    "calsim_base_path = find_calsim_base_path(base_dir)\n",
    "\n",
    "# Load groundwater CSV from correct folder\n",
    "csv_path = os.path.join(\n",
    "    calsim_base_path,\n",
    "    \"Scenarios\",\n",
    "    \"Group_Data_Extraction\",\n",
    "    \"GroundWater_DataMonthly.csv\"\n",
    ")\n",
    "\n",
    "# Plot output directory\n",
    "plot_output_dir = os.path.join(\n",
    "    calsim_base_path,\n",
    "    \"Scenarios\",\n",
    "    \"Performance_Metrics\",\n",
    "    \"Groundwater\"\n",
    ")\n",
    "os.makedirs(plot_output_dir, exist_ok=True)\n",
    "\n",
    "# === Load data ===\n",
    "gw1_df_filtered = pd.read_csv(\n",
    "    csv_path,\n",
    "    header=[0, 1, 2, 3, 4],\n",
    "    index_col=0,\n",
    "    parse_dates=True\n",
    ")\n",
    "\n",
    "# === Constants ===\n",
    "baseline_scenario = \"s0011\"\n",
    "end_year_override = {\"s0006\", \"s0007\", \"s0008\", \"s0009\", \"s0010\"}\n",
    "drop_threshold = 1000\n",
    "start_year = 1960\n",
    "\n",
    "# === Function ===\n",
    "def plot_trendline_comparison(scenario_code, baseline_code=baseline_scenario, save_dir=None):\n",
    "    scenario_data = {}\n",
    "    baseline_data = {}\n",
    "\n",
    "    for col in gw1_df_filtered.columns:\n",
    "        model, var, typ, timestep, unit = col\n",
    "        if not re.match(r'^(WBA\\d+)_s\\d{4}$', var):\n",
    "            continue\n",
    "        wba_id, scenario = var.split(\"_\")\n",
    "        if scenario == scenario_code:\n",
    "            scenario_data[wba_id] = gw1_df_filtered[col]\n",
    "        elif scenario == baseline_code:\n",
    "            baseline_data[wba_id] = gw1_df_filtered[col]\n",
    "\n",
    "    shared_wbas = sorted(set(scenario_data.keys()) & set(baseline_data.keys()))\n",
    "    if not shared_wbas:\n",
    "        print(f\"No shared WBAs for {scenario_code} and {baseline_code}\")\n",
    "        return\n",
    "\n",
    "    ncols = 3\n",
    "    nrows = math.ceil(len(shared_wbas) / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5 * ncols, 3 * nrows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, wba in enumerate(shared_wbas):\n",
    "        # Scenario time series\n",
    "        ts = scenario_data[wba]\n",
    "        ts = ts[ts.index >= pd.Timestamp(f\"{start_year}-01-01\")].dropna()\n",
    "        if scenario_code in end_year_override:\n",
    "            ts = ts[ts.index < pd.Timestamp(\"2016-01-01\")]\n",
    "        else:\n",
    "            diffs = ts.diff()\n",
    "            drop_indices = diffs[diffs < -drop_threshold].index\n",
    "            if not drop_indices.empty:\n",
    "                ts = ts[ts.index <= drop_indices[0]]\n",
    "\n",
    "        # Baseline time series\n",
    "        ts_base = baseline_data[wba]\n",
    "        ts_base = ts_base[ts_base.index >= pd.Timestamp(f\"{start_year}-01-01\")].dropna()\n",
    "        if baseline_code in end_year_override:\n",
    "            ts_base = ts_base[ts_base.index < pd.Timestamp(\"2016-01-01\")]\n",
    "        else:\n",
    "            diffs = ts_base.diff()\n",
    "            drop_indices = diffs[diffs < -drop_threshold].index\n",
    "            if not drop_indices.empty:\n",
    "                ts_base = ts_base[ts_base.index <= drop_indices[0]]\n",
    "\n",
    "        # Trendline for scenario\n",
    "        x = (ts.index - ts.index[0]).days / 365.25\n",
    "        y = ts.values\n",
    "        slope, intercept = np.polyfit(x, y, 1)\n",
    "        trend = slope * x + intercept\n",
    "\n",
    "        # Trendline for baseline\n",
    "        x_base = (ts_base.index - ts_base.index[0]).days / 365.25\n",
    "        y_base = ts_base.values\n",
    "        slope_base, intercept_base = np.polyfit(x_base, y_base, 1)\n",
    "        trend_base = slope_base * x_base + intercept_base\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.plot(ts.index, trend, color=\"red\", linestyle=\"--\", label=f\"{scenario_code} trend\")\n",
    "        ax.plot(ts_base.index, trend_base, color=\"black\", linestyle=\"--\", label=f\"{baseline_code} trend\")\n",
    "        ax.set_title(wba)\n",
    "        ax.set_xlim(pd.Timestamp(\"1960-01-01\"), pd.Timestamp(\"2021-12-31\"))\n",
    "        ax.set_ylabel(\"FT\")\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Display slope values inside each subplot\n",
    "        ax.text(0.01, 0.95, f\"{scenario_code} slope: {slope:.6f}\", transform=ax.transAxes, fontsize=8, color=\"red\", verticalalignment='top')\n",
    "        ax.text(0.01, 0.85, f\"{baseline_code} slope: {slope_base:.6f}\", transform=ax.transAxes, fontsize=8, color=\"black\", verticalalignment='top')\n",
    "\n",
    "        # Add legend to bottom right to avoid overlap\n",
    "        ax.legend(loc=\"lower right\", fontsize=7, frameon=True)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(f\"Trendline Comparison: {scenario_code} vs {baseline_code}\", fontsize=15)\n",
    "    fig.tight_layout(rect=[0, 0.04, 1, 0.96])  # extra space for title\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"{scenario_code}_vs_{baseline_code}_trendlines.png\")\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"✓ Saved: {save_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# === Call the function for each scenario ===\n",
    "all_scenarios = [f\"s{str(i).zfill(4)}\" for i in [1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,18,19,20,21]]  # exclude s0011\n",
    "for sc in all_scenarios:\n",
    "    plot_trendline_comparison(scenario_code=sc, save_dir=plot_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ecde2b-dd0f-4a2b-88a4-5410a72a017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\".\\shapefiles (1)\"\n",
    "shp_files = [f for f in os.listdir(folder_path) if f.endswith(\".shp\")]\n",
    "for f in shp_files:\n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7e719-a233-4be3-b656-fcfac0d858a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644c308-88ea-4009-ac61-ccc493b84be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# === Load shapefile using relative path ===\n",
    "notebook_dir = os.path.abspath(\".\")\n",
    "shapefile_path = os.path.join(notebook_dir, \"shapefiles (1)\", \"i12_CalSim3Model_WaterBudgetAreas_20221021.shp\")\n",
    "wba_shp = gpd.read_file(shapefile_path)\n",
    "wba_shp[\"WBA_ID\"] = wba_shp[\"WBA_ID\"].str.strip()\n",
    "\n",
    "# === Locate CalSim3_Model_Runs directory ===\n",
    "def find_calsim_base_path(start_path, target_folder=\"CalSim3_Model_Runs\"):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        parent = os.path.dirname(current_path)\n",
    "        candidate = os.path.join(parent, target_folder)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        if parent == current_path:\n",
    "            raise FileNotFoundError(f\"{target_folder} not found above {start_path}\")\n",
    "        current_path = parent\n",
    "\n",
    "# === Load tier assignment CSV ===\n",
    "base_dir = os.path.abspath(\".\")\n",
    "calsim_base_path = find_calsim_base_path(base_dir)\n",
    "tier_output_dir = os.path.join(\n",
    "    calsim_base_path,\n",
    "    \"Scenarios\",\n",
    "    \"Performance_Metrics\",\n",
    "    \"Tiered_Outcome_Measures\",\n",
    "    \"Groundwater\"\n",
    ")\n",
    "\n",
    "\n",
    "# === Fix WBA column names to match shapefile format ===\n",
    "new_columns = {}\n",
    "for col in tier_df.columns:\n",
    "    if col.startswith(\"WBA\"):\n",
    "        suffix = col[3:]\n",
    "        if suffix.isdigit():\n",
    "            new_columns[col] = suffix.zfill(2)\n",
    "        else:\n",
    "            digits = ''.join(filter(str.isdigit, suffix)).zfill(2)\n",
    "            letter = ''.join(filter(str.isalpha, suffix))\n",
    "            new_columns[col] = digits + letter\n",
    "tier_df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "# === Define atlas-style muted tier colors ===\n",
    "tier_colors = {\n",
    "    1: \"#8FBBD9\",  # soft dusty blue\n",
    "    2: \"#B5CDA3\",  # olive green\n",
    "    3: \"#E6C27A\",  # warm khaki\n",
    "    4: \"#D97B6D\",  # soft brick red (worst)\n",
    "}\n",
    "\n",
    "\n",
    "# === Vertical shifts for selected WBA labels ===\n",
    "label_shifts = {\n",
    "    \"17STOT\": 0.015,\n",
    "    \"71TOT\": 0.015,\n",
    "    \"22TOT\": 0.015,\n",
    "    \"50TOT\": -0.015,\n",
    "    \"21TOT\": -0.015,\n",
    "    \"12TOT\": -0.015,\n",
    "}\n",
    "\n",
    "# === Plot and save maps for each scenario (except baseline) ===\n",
    "for scenario in tier_df.index:\n",
    "    if scenario == 's0011':\n",
    "        continue\n",
    "\n",
    "    # Map tier data to shapefile\n",
    "    tier_series = tier_df.loc[scenario]\n",
    "    tier_map = wba_shp.copy()\n",
    "    tier_map[\"Tier\"] = tier_map[\"WBA_ID\"].map(tier_series.to_dict())\n",
    "\n",
    "    # Plot base\n",
    "    fig, ax = plt.subplots(figsize=(8, 10))\n",
    "    for tier_val, color in tier_colors.items():\n",
    "        subset = tier_map[tier_map[\"Tier\"] == tier_val]\n",
    "        if not subset.empty:\n",
    "            subset.plot(\n",
    "                ax=ax,\n",
    "                color=color,\n",
    "                edgecolor='black',\n",
    "                linewidth=0.3,\n",
    "                label=f\"Tier {tier_val}\"\n",
    "            )\n",
    "\n",
    "    # === Add WBA_ID labels at centroids with optional vertical shift ===\n",
    "    for idx, row in tier_map.iterrows():\n",
    "        if pd.notna(row[\"Tier\"]):\n",
    "            x, y = row.geometry.centroid.x, row.geometry.centroid.y\n",
    "            wba_id = row[\"WBA_ID\"]\n",
    "            shift = label_shifts.get(wba_id, 0)\n",
    "            ax.text(x, y + shift, wba_id, fontsize=7, weight='bold', ha='center')\n",
    "\n",
    "    ax.set_title(f\"Groundwater Tiers for Scenario {scenario}\", fontweight=\"bold\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # Add legend\n",
    "    legend_handles = [mpatches.Patch(color=color, label=f\"Tier {tier}\")\n",
    "                      for tier, color in tier_colors.items()]\n",
    "    ax.legend(handles=legend_handles, title=\"Tier\", loc=\"lower left\", frameon=True)\n",
    "\n",
    "    # Save and show\n",
    "    output_path = os.path.join(tier_output_dir, f\"GroundwaterTiers_{scenario}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✓ Saved map for {scenario} to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a49d7a-bd30-4a03-b26b-91d38d191f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "def pad_index(idx):\n",
    "    import re\n",
    "    if isinstance(idx, str):\n",
    "        m = re.match(r'^(\\d{1,2})([A-Z]*)$', idx)  # Matches numbers + optional suffix\n",
    "        if m:\n",
    "            num = int(m.group(1))\n",
    "            suffix = m.group(2)\n",
    "            return f\"{num:02d}{suffix}\"\n",
    "    return idx  # If it doesn't match, return as is\n",
    "\n",
    "notebook_dir = os.path.abspath(\".\")\n",
    "shapefile_path = os.path.join(notebook_dir, \"shapefiles (1)\", \"i12_CalSim3Model_WaterBudgetAreas_20221021.shp\")\n",
    "wba_shp = gpd.read_file(shapefile_path)\n",
    "wba_shp[\"WBA_ID\"] = wba_shp[\"WBA_ID\"].str.strip().str.upper()\n",
    "\n",
    "# print (\" Shapefile contents:\")\n",
    "# print(wba_shp)\n",
    "# print(\"Baseline data:\")\n",
    "# print(baseline_data)\n",
    "\n",
    "\n",
    "baseline_data.index = (\n",
    "    baseline_data.index\n",
    "    .str.replace(\"WBA\", \"\", regex=False)\n",
    "    .str.replace(\":TOT\", \"\", regex=False)\n",
    "    .str.replace(\"TOT\", \"\", regex=False)\n",
    "    .str.lstrip(\"0\")\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    ")\n",
    "\n",
    "# print(\"Baseline data after modifications:\")\n",
    "# print(baseline_data)\n",
    "baseline_data.index = baseline_data.index.map(pad_index)\n",
    "intersect = set(wba_shp[\"WBA_ID\"]) & set(baseline_data.index)\n",
    "# print(\"set(wba_shp[WBA_ID]):\")\n",
    "# print(set(wba_shp[\"WBA_ID\"]))\n",
    "# print(\"set(baseline_data.index):\")\n",
    "# print(set(baseline_data.index))\n",
    "# print(\"intersect:\")\n",
    "# print(intersect)\n",
    "\n",
    "slope_rank = baseline_data.rank().astype(int)\n",
    "slope_map = wba_shp.copy()\n",
    "slope_map[\"Slope\"] = slope_map[\"WBA_ID\"].map(baseline_data.squeeze().to_dict())\n",
    "slope_map[\"SlopeRank\"] = slope_map[\"WBA_ID\"].map(slope_rank.squeeze().to_dict())\n",
    "\n",
    "\n",
    "num_ranks = slope_rank.nunique()\n",
    "cmap = plt.colormaps.get_cmap(\"coolwarm_r\").resampled(num_ranks)\n",
    "norm = mcolors.Normalize(vmin=1, vmax=num_ranks)\n",
    "\n",
    "def find_calsim_base_path(start_path, target_folder=\"CalSim3_Model_Runs\"):\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        parent = os.path.dirname(current_path)\n",
    "        candidate = os.path.join(parent, target_folder)\n",
    "        if os.path.isdir(candidate):\n",
    "            return candidate\n",
    "        if parent == current_path:\n",
    "            raise FileNotFoundError(f\"{target_folder} not found above {start_path}\")\n",
    "        current_path = parent\n",
    "\n",
    "base_dir = os.path.abspath(\".\")\n",
    "calsim_base_path = find_calsim_base_path(base_dir)\n",
    "output_dir = os.path.join(\n",
    "    calsim_base_path,\n",
    "    \"Scenarios\",\n",
    "    \"Performance_Metrics\",\n",
    "    \"Tiered_Outcome_Measures\",\n",
    "    \"Groundwater\"\n",
    ")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Plot map ===\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "slope_map.plot(\n",
    "    column=\"SlopeRank\",\n",
    "    cmap=cmap,\n",
    "    linewidth=0.3,\n",
    "    edgecolor='black',\n",
    "    ax=ax,\n",
    "    legend=False,\n",
    "    norm=norm\n",
    ")\n",
    "\n",
    "# === Label WBAs ===\n",
    "for idx, row in slope_map.iterrows():\n",
    "    if pd.notna(row[\"SlopeRank\"]):\n",
    "        x, y = row.geometry.centroid.x, row.geometry.centroid.y\n",
    "        wba_id = row[\"WBA_ID\"]\n",
    "        shift = label_shifts.get(wba_id, 0)\n",
    "        ax.text(x, y + shift, wba_id, fontsize=7, weight='bold', ha='center')\n",
    "\n",
    "# === Colorbar with slope values ===\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm._A = []\n",
    "rank_to_slope = slope_map.dropna(subset=[\"Slope\", \"SlopeRank\"]).groupby(\"SlopeRank\")[\"Slope\"].mean().sort_index()\n",
    "tick_locs = list(rank_to_slope.index)\n",
    "tick_labels = [f\"{s:.6f}\" for s in rank_to_slope.values]\n",
    "\n",
    "cbar = fig.colorbar(sm, ax=ax, orientation=\"vertical\", ticks=tick_locs)\n",
    "cbar.set_label(\"Slope Value\")\n",
    "cbar.ax.yaxis.set_major_locator(FixedLocator(tick_locs))\n",
    "cbar.ax.yaxis.set_major_formatter(FixedFormatter(tick_labels))\n",
    "\n",
    "# === Finalize layout ===\n",
    "ax.set_title(\"Slope Rank Map for Scenario s0011 (Baseline)\", fontweight=\"bold\")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# === Save and show ===\n",
    "output_path = os.path.join(output_dir, \"SlopeRank_s0011.png\")\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\" Saved slope rank map to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa5787-6188-4eae-904c-0b00c06378e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812624b-279d-4c85-a1df-8d9ffe120113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
